{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with Graph Retrieval\n",
    "This notebook impliments graph context retrieval and augmenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- issues with retrieving relevant nodes.. many things comes back as \"linear algebra\" when finding most similiar node\n",
    "- generated KG has node that is irrelevant e.g. \"a\". tried cleaning the graph of common words... mediocre results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#Initialize the model endpoint\n",
    "HOST_URL_INF = \":8080\"\n",
    "MAX_NEW_TOKENS = 2200\n",
    "\n",
    "TEMPERATURE = 0.2\n",
    "TIMEOUT = 300\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=HOST_URL_INF,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature = TEMPERATURE,\n",
    "    timeout=TIMEOUT,\n",
    ")\n",
    "\n",
    "model_name = \"dunzhang/stella_en_1.5B_v5\" #\"BAAI/bge-small-en-v1.5\" #dunzhang/stella_en_1.5B_v5\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embd = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a company that specializes in machine learning.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"In 10 words, huggingface is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG - Graph reasoning using GraphReasoning by MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "root_path = \"c:\\\\Users\\\\jonathan.kasprisin\\\\gitlab\\\\DNoK_GraphRAG\"\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphReasoning_Mod.graph_tools import *\n",
    "from GraphReasoning_Mod.utils import *\n",
    "from GraphReasoning_Mod.graph_generation import *\n",
    "from GraphReasoning_Mod.graph_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert tutor in mathematics and linear algebra, specializing in personalized and context-aware explanations. \n",
      "Your goal is to provide clear, relevant, and engaging responses tailored to the student’s profile, retrieved context, and their request. \n",
      "Use the information provided to adapt your explanation to their background, strengths, weaknesses, and preferences.\n",
      "\n",
      "### Student Profile:\n",
      "\n",
      "        Background: Recent college graduate with a degree in Business Administration.\n",
      "        Strengths: Strong organizational and project management skills.\n",
      "        Weaknesses: Limited mathematical background; no prior programming experience.\n",
      "        Preferences: Prefers real-world applications, interactive learning, and visualizations.\n",
      "        Prior Course History: \n",
      "        - Introduction to Business Mathematics\n",
      "        - Basic Statistics for Managers\n",
      "    \n",
      "\n",
      "### Retrieved Context:\n",
      "\n",
      "\n",
      "### Student Request:\n",
      "Help me understand how eigenvalues relate to matrix transformations. Provide content that visually explains this concept and its applications in data analysis.\n",
      "\n",
      "### Instructions for Response:\n",
      "1. Start with a concise text summary addressing the student’s request directly.\n",
      "2. Provide a detailed explanation that aligns with their preferences (e.g., visual aids, practical examples, or technical depth).\n",
      "3. Relate your explanation to the student’s strengths while addressing their weaknesses constructively.\n",
      "4. Suggest specific content from the retrieved context to help the student further examine relationships between the topics.\n",
      "\n",
      "### Example Structure for Your Response:\n",
      "**1. Summary:** A short, focused answer to the request.  \n",
      "**2. Detailed Explanation:** An in-depth response tailored to the student’s background and the retrieved context. Include examples as needed.   \n",
      "**3. Specific Resources:** Specifics of where to access material that supports the detailed explanation.  \n",
      "  \n",
      "\n",
      "Remember to always prioritize clarity and ensure the response is aligned with the student’s knowledge level and preferences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load prompt templates and testing student profiles\n",
    "from utils.prompt_templates import chatbot_prompt_template\n",
    "from utils.test_case_data   import student_1, student_2, student_3\n",
    "\n",
    "students = [student_1, student_2, student_3]\n",
    "\n",
    "#test prompt template\n",
    "prompt_test = chatbot_prompt_template.format(\n",
    "        # student_name=student_1[\"name\"],\n",
    "        profile=student_1[\"profile\"],\n",
    "        context=\"\",\n",
    "        request=student_1[\"requests\"][0],\n",
    "    )\n",
    "\n",
    "print(prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Graph: GR_kg_no_refine_final\n",
      "-->Number of nodes: 3484, Number of edges: 9841\n",
      "-->example node data: [('C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', {'group': 1, 'color': '#57dbc2', 'size': 2497})]\n",
      "-->example edge data: [('C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'a', {'title': 'is source document of', 'weight': 1.0})]\n",
      "\n",
      "-->number of embeddings loaded: 3484\n",
      "\n",
      "Loaded Graph: GR_kg_no_refine3_1.0\n",
      "-->Number of nodes: 77039, Number of edges: 62284\n",
      "-->example node data: [('cartesian product of vector spaces', {'group': 1, 'color': '#57db89', 'size': 1})]\n",
      "-->example edge data: [('cartesian product of vector spaces', 'ordered pairs', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)', 'author': 'Gilbert Strang', 'start_index': 1497}\", 'weight': 1.0})]\n",
      "\n",
      "-->number of embeddings loaded: 77039\n",
      "\n",
      "Loaded Graph: GR_kg_no_refine3_0.95\n",
      "-->Number of nodes: 37709, Number of edges: 58504\n",
      "-->example node data: [('product of transformation matrices', {'group': 891, 'color': '#dbcc57', 'size': 1})]\n",
      "-->example edge data: [('product of transformation matrices', 'euclidean lines', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)', 'author': 'Gilbert Strang', 'start_index': 1497}\", 'weight': 1.0})]\n",
      "\n",
      "-->number of embeddings loaded: 37709\n",
      "\n",
      "Loaded Graph: GR_kg_no_refine3_0.85\n",
      "-->Number of nodes: 18933, Number of edges: 33153\n",
      "-->example node data: [('product of transformation matrices', {'group': 891, 'color': '#dbcc57', 'size': 1})]\n",
      "-->example edge data: [('product of transformation matrices', 'euclidean lines', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)', 'author': 'Gilbert Strang', 'start_index': 1497}\", 'weight': 1.0})]\n",
      "\n",
      "-->number of embeddings loaded: 18933\n",
      "\n",
      "Loaded Graph: GR_kg_no_refine3_0.75\n",
      "-->Number of nodes: 13596, Number of edges: 24305\n",
      "-->example node data: [('product of transformation matrices', {'group': 891, 'color': '#dbcc57', 'size': 1})]\n",
      "-->example edge data: [('product of transformation matrices', 'euclidean lines', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)', 'author': 'Gilbert Strang', 'start_index': 1497}\", 'weight': 1.0})]\n",
      "\n",
      "-->number of embeddings loaded: 13596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "graph_file_names = {\n",
    "    \"GR_kg_no_refine_final\": \"final_augmented_graph.graphml\",\n",
    "    # \"GR_kg_w_refine_final\": \"final_augmented_graph.graphml\",\n",
    "    # \"langchain_kg\": \"langchain_full_kg.graphml\",\n",
    "    # \"sme_kg\": \"DNoKv3_forGR.graphml\",\n",
    "    \"GR_kg_no_refine3_1.0\": \"final_augmented_graph.graphml\",\n",
    "    \"GR_kg_no_refine3_0.95\": \"0.95threshold_graphML_simplified.graphml\",\n",
    "    \"GR_kg_no_refine3_0.85\" : \"0.85threshold_graphML_simplified.graphml\",\n",
    "    \"GR_kg_no_refine3_0.75\" : \"0.75threshold_graphML_simplified.graphml\",\n",
    "}\n",
    "\n",
    "\n",
    "graph_dirs = {\n",
    "    \"GR_kg_no_refine_final\": \"./data/generated_graphs/GR_no_refine/\",\n",
    "    # \"GR_kg_w_refine_final\": \"./data/generated_graphs/GR_w_refine/\",\n",
    "    # \"langchain_kg\": \"./data/generated_graphs/langchain_KG/\",\n",
    "    # \"sme_kg\": \"./data/generated_graphs/SME_graph/\" ,\n",
    "    \"GR_kg_no_refine3_1.0\": \"./data/generated_graphs/GR_no_refine3/\",\n",
    "    \"GR_kg_no_refine3_0.95\": \"./data/generated_graphs/GR_no_refine3/\",\n",
    "    \"GR_kg_no_refine3_0.85\" : \"./data/generated_graphs/GR_no_refine3/\",\n",
    "    \"GR_kg_no_refine3_0.75\" : \"./data/generated_graphs/GR_no_refine3/\",\n",
    "}\n",
    "\n",
    "embd_file_names = {\n",
    "    \"GR_kg_no_refine_final\": \"embeddings.pkl\",\n",
    "    # \"GR_kg_w_refine_final\": \"embeddings.pkl\",\n",
    "    # \"langchain_kg\": \"embeddings.pkl\",\n",
    "    # \"sme_kg\": \"embeddings.pkl\",\n",
    "    \"GR_kg_no_refine3_1.0\": \"embeddings.pkl\",\n",
    "    \"GR_kg_no_refine3_0.95\": \"GR_kg_no_refine3_0.95_embeddings.pkl\", #\"0.95threshold_embeddings.pkl\",\n",
    "    \"GR_kg_no_refine3_0.85\" : \"GR_kg_no_refine3_0.85_embeddings.pkl\", #\"0.85threshold_embeddings.pkl\",\n",
    "    \"GR_kg_no_refine3_0.75\" : \"GR_kg_no_refine3_0.75_embeddings.pkl\", #\"0.75threshold_embeddings.pkl\",\n",
    "}\n",
    "\n",
    "\n",
    "#dictionary of graphs\n",
    "graphs_paths = {}\n",
    "embeddings_paths = {}\n",
    "\n",
    "for graph_name, graph_dir in graph_dirs.items():\n",
    "    graph_path = os.path.join(graph_dir, graph_file_names[graph_name])\n",
    "    embeddings_path = os.path.join(graph_dir, embd_file_names[graph_name])\n",
    "    graphs_paths[graph_name] = graph_path\n",
    "    embeddings_paths[graph_name] = embeddings_path\n",
    "\n",
    "\n",
    "graph_dict = {}\n",
    "embds_dict = {}\n",
    "\n",
    "for graph_name, graph_path in graphs_paths.items():\n",
    "    embeddings_path = embeddings_paths[graph_name]\n",
    "    try:\n",
    "        graph= nx.read_graphml(graph_path)\n",
    "        graph_dict[graph_name] = graph\n",
    "\n",
    "        # Print info about the graph\n",
    "        print(f\"Loaded Graph: {graph_name}\")\n",
    "        print(f\"-->Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
    "        print(f\"-->example node data: {list(graph.nodes(data=True))[:1]}\")\n",
    "        print(f\"-->example edge data: {list(graph.edges(data=True))[:1]}\\n\")\n",
    "\n",
    "        if os.path.exists(embeddings_path):\n",
    "            with open(embeddings_path, 'rb') as f:\n",
    "                existing_node_embeddings = pickle.load(f)\n",
    "        else:\n",
    "            existing_node_embeddings = generate_node_embeddings(graph, embd)\n",
    "            with open(embeddings_path, 'wb') as f:\n",
    "                pickle.dump(existing_node_embeddings, f)\n",
    "        embds_dict[graph_name]= existing_node_embeddings\n",
    "        print(f\"-->number of embeddings loaded: {len(existing_node_embeddings)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load graph {graph_name} from {graph_path}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test changing edge data\n",
    "import ast \n",
    "\n",
    "def reduce_metadata(G):\n",
    "    # Iterate over all edges and remove 'start_index' from 'metadata'\n",
    "    for u, v, data in tqdm(G.edges(data=True), desc=\"Processing edges\", total= G.number_of_edges()):\n",
    "        if 'metadata' in data:\n",
    "            try:\n",
    "                # Convert string to dictionary\n",
    "                metadata_dict = ast.literal_eval(data['metadata'])\n",
    "                # Remove the 'start_index' and author keys if they exists\n",
    "                metadata_dict.pop('start_index', None)\n",
    "                metadata_dict.pop('author', None)\n",
    "                # Update the metadata string\n",
    "                data['metadata'] = str(metadata_dict)\n",
    "            except (ValueError, SyntaxError):\n",
    "                print(f\"Failed to parse metadata for edge ({u}, {v}): {data['metadata']}\")\n",
    "\n",
    "# # Load your graph\n",
    "# reduce_metadata(graph_dict[\"GR_kg_no_refine3_0.85\"])\n",
    "# print(f\"-->example node data: {list(graph_dict[\"GR_kg_no_refine3_0.85\"].edges(data=True))[:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c165f0cb814cb094f2ac3ad763c8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing edges:   0%|          | 0/9841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb496982f3014f8485aa9495570475cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing edges:   0%|          | 0/62284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0050df1694ad48538ba47f5f3b9288b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing edges:   0%|          | 0/58504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ee29242d1c4bcaaefd1d0a284481df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing edges:   0%|          | 0/33153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40707f67f07475eb3c9ec747e4dee53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing edges:   0%|          | 0/24305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for g in graph_dict:\n",
    "    reduce_metadata(graph_dict[g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm(system_prompt='You are a helpful assistant.', \n",
    "                         prompt=\"Hello.\",temperature=0.3,\n",
    "                         max_tokens=1024, timeout=180 \n",
    "                         ):\n",
    "    \n",
    "    llm = HuggingFaceEndpoint(\n",
    "        endpoint_url=HOST_URL_INF,\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature = temperature,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    prompt= system_prompt + prompt\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "#usage\n",
    "#print(generate_llm(prompt=\"What is spider silk?\",temperature=0.3, max_tokens=1024, timeout=180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_info(graph):\n",
    "    if graph.is_directed():\n",
    "        print(\"directed graph \")\n",
    "    else:\n",
    "        print(\"undirected graph\")\n",
    "    print(f\"-->Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
    "    print(f\"-->example node data: {list(graph.nodes(data=True))[:1]}\")\n",
    "    print(f\"-->example edge data: {list(graph.edges(data=True))[:1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph(graph, existing_node_embeddings):\n",
    "    \"\"\" \n",
    "    Removes common word nodes from graph and corresponding embeddings\n",
    "    \"\"\"\n",
    "    # Define the list of common words to be removed\n",
    "    common_words = {\n",
    "        # Articles\n",
    "        'a', 'an', 'the',\n",
    "\n",
    "        # Pronouns\n",
    "        'i', 'me', 'you', 'he', 'she', 'it', 'we', 'us', 'they', 'them',\n",
    "        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours',\n",
    "        'hers', 'theirs',\n",
    "\n",
    "        # Conjunctions\n",
    "        'and', 'but', 'or', 'nor', 'for', 'yet', 'so',\n",
    "\n",
    "        # Prepositions\n",
    "        'about', 'above', 'across', 'after', 'against', 'along', 'among',\n",
    "        'around', 'at', 'before', 'behind', 'below', 'beneath', 'beside',\n",
    "        'between', 'beyond', 'by', 'down', 'during', 'except', 'for', 'from',\n",
    "        'in', 'inside', 'into', 'near', 'of', 'off', 'on', 'out', 'outside',\n",
    "        'over', 'through', 'to', 'toward', 'under', 'until', 'up', 'upon',\n",
    "        'with', 'within', 'without',\n",
    "\n",
    "        # Auxiliary (Helping) Verbs\n",
    "        'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been', 'do', 'does',\n",
    "        'did', 'have', 'has', 'had', 'will', 'would', 'shall', 'should', 'can',\n",
    "        'could', 'may', 'might', 'must',\n",
    "\n",
    "        # Adverbs\n",
    "        'not', 'no', 'very', 'too', 'just', 'only',\n",
    "\n",
    "        # Other Common Words\n",
    "        'all', 'any', 'both', 'each', 'every', 'few', 'many', 'more', 'most',\n",
    "        'other', 'some', 'such', 'that', 'this', 'these', 'those', 'which',\n",
    "        'what', 'who', 'whom', 'whose', 'how', 'when', 'where', 'why'\n",
    "    }\n",
    "\n",
    "    # Load the graph from a GraphML file\n",
    "    graph_clean = graph\n",
    "\n",
    "    # Print initial number of nodes and edge data for verification\n",
    "    print(f\"Initial number of nodes: {graph_clean.number_of_nodes()}\")\n",
    "    print(f\"Initial number of edges: {graph_clean.number_of_edges()}\")\n",
    "\n",
    "    # Identify nodes to remove based on common words\n",
    "    nodes_to_remove = [node for node in graph_clean.nodes if str(node).lower() in common_words]\n",
    "\n",
    "    # Remove identified nodes from the graph\n",
    "    graph_clean.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "    # # Save the cleaned graph back to a new GraphML file\n",
    "    # cleaned_graphml_path = \"cleaned_graph.graphml\"\n",
    "    # nx.write_graphml(graph_clean, cleaned_graphml_path)\n",
    "\n",
    "    # Print final node and edge data for verification\n",
    "    print(f\"Cleaned number of nodes: {graph_clean.number_of_nodes()}\")\n",
    "    print(f\"Cleaned number of edges: {graph_clean.number_of_edges()}\")\n",
    "    print(f\"Final node data: {list(graph_clean.nodes(data=True))[:1]}\")\n",
    "    print(f\"Final edge data: {list(graph_clean.edges(data=True))[:1]}\")\n",
    "    # print(f\"Cleaned graph saved to: {cleaned_graphml_path}\")\n",
    "\n",
    "    # Remove nodes from the embedding file if in nodes to remove\n",
    "    existing_node_embeddings = {node: embedding for node, embedding in existing_node_embeddings.items() if node not in nodes_to_remove}\n",
    "    print(f\"Number of node embeddings: {len(existing_node_embeddings)}\\n\")\n",
    "\n",
    "    return graph_clean, existing_node_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Override module functions\n",
    "TODO: update and re-integrate into GraphReasoning_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from GraphReasoning_Mod...?\n",
    "def find_path( G, node_embeddings,  embedding_object, keyword_1 = \"\", keyword_2 = \"\", \n",
    "            verbatim=True, second_hop=False,data_dir='./output_files/', similarity_fit_ID_node_1=0,\n",
    "            similarity_fit_ID_node_2=0,save_files=False):\n",
    "    \"\"\"\n",
    "    This function finds the shortest path with two hops between two keywords in a graph. \n",
    "    It first finds the best fitting nodes for each keyword, if the the best fitting node is the same node it takes the second best for the second keyword.\n",
    "    Then finds the shortest path between these nodes.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    best_node_1, best_similarity_1= find_best_fitting_node_list(\n",
    "        keyword_1, node_embeddings, embedding_object, max (5, similarity_fit_ID_node_1+1)\n",
    "    )[similarity_fit_ID_node_1]\n",
    "    \n",
    "    if verbatim:\n",
    "        print(f\"{similarity_fit_ID_node_1}nth best fitting node for '{keyword_1}': '{best_node_1}' with similarity: {best_similarity_1}\")\n",
    "    \n",
    "    best_node_2, best_similarity_2 = find_best_fitting_node_list(\n",
    "        keyword_2, node_embeddings, embedding_object,  max (5, similarity_fit_ID_node_2+1)\n",
    "    )[similarity_fit_ID_node_2]\n",
    "\n",
    "    if verbatim:\n",
    "        print(f\"{similarity_fit_ID_node_2}nth best fitting node for '{keyword_2}': '{best_node_2}' with similarity: {best_similarity_2}\")\n",
    "\n",
    "    if best_node_1 == best_node_2:\n",
    "        if verbatim: \n",
    "            print(f\"Warning: The two keywords are the same: '{keyword_1}' and '{keyword_2}'\")\n",
    "        best_node_2, best_similarity_2 = find_best_fitting_node_list(\n",
    "            keyword_2, node_embeddings, embedding_object,  max (5, similarity_fit_ID_node_2+1)\n",
    "        )[similarity_fit_ID_node_2+1]\n",
    "    \n",
    "    path, path_graph , shortest_path_length = find_shortest_path_with2hops (\n",
    "        G,source=best_node_1, target=best_node_2, second_hop=second_hop, verbatim=verbatim, data_dir=data_dir,save_files=save_files,\n",
    "    )\n",
    "    \n",
    "    return (best_node_1, best_similarity_1, best_node_2, best_similarity_2), path, path_graph, shortest_path_length #, fname, graph_GraphML\n",
    "\n",
    "#from GraphReasoning_Mod.graph_analysis\n",
    "def find_shortest_path_with2hops (G, source='graphene', target='complexity',\n",
    "                                 second_hop=True,#otherwise just neighbors\n",
    "                                  verbatim=True,data_dir='./output_files/', save_files=True,\n",
    "                                 ):\n",
    "    \"\"\" \n",
    "    Returns the shortest path between two nodes in a graph, along with a subgraph containing all nodes within 2 hops.\n",
    "    Note: Returns None for all outputs if no path is found between the source and target nodes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the shortest path between two nodes\n",
    "        path = nx.shortest_path(G, source=source, target=target)\n",
    "    except Exception as e:\n",
    "        if verbatim:\n",
    "            print(f\"No path found between '{source}' and '{target}'\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Initialize a set to keep track of all nodes within 2 hops\n",
    "    nodes_within_2_hops = set(path)\n",
    "    \n",
    "    # Expand the set to include all neighbors within 2 hops of the path nodes\n",
    "    for node in path:\n",
    "        for neighbor in G.neighbors(node):\n",
    "            nodes_within_2_hops.add(neighbor)\n",
    "            # Include the neighbors of the neighbor (2 hops)\n",
    "\n",
    "            if second_hop:\n",
    "                for second_neighbor in G.neighbors(neighbor):\n",
    "                    nodes_within_2_hops.add(second_neighbor)\n",
    "    \n",
    "    # Create a subgraph for the nodes within 2 hops\n",
    "    path_graph = G.subgraph(nodes_within_2_hops)\n",
    "\n",
    "    if save_files:\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        \n",
    "        # nt = Network('500px', '1000px', notebook=True)\n",
    "        \n",
    "        # # Add nodes and edges from the subgraph to the Pyvis network\n",
    "        # nt.from_nx(path_graph)\n",
    "        \n",
    "        fname=f'shortest_path_2hops_{source[:10]}_{target[:10]}'\n",
    "        #remove all characters that will cause issues with the file name\n",
    "        fname = ''.join(e for e in fname if e.isalnum() or e in ['_', '.', '/', '\\\\'])\n",
    "\n",
    "        # nt.show(f\"{data_dir}/{fname}.html\")\n",
    "        # if verbatim:\n",
    "        #     print(f\"HTML visualization: {fname}\")\n",
    "\n",
    "        graph_GraphML = f'{data_dir}/{fname}.graphml'\n",
    "        nx.write_graphml(path_graph, graph_GraphML)\n",
    "        if verbatim:\n",
    "            print(f\"GraphML file: {graph_GraphML}\")  \n",
    "    else:\n",
    "        fname=None\n",
    "        graph_GraphML=None\n",
    "        \n",
    "    shortest_path_length = len(path) - 1  # As path length is number of edges\n",
    "    \n",
    "    return path, path_graph , shortest_path_length, #fname, graph_GraphML\n",
    "\n",
    "#from GraphReasoning_Mod.graph_tools\n",
    "def find_best_fitting_node_list(keyword, embeddings, embedding_object, N_samples=5):\n",
    "    \"\"\"\n",
    "    Find the top N_samples nodes with the highest similarity to the keyword.\n",
    "\n",
    "    Parameters:\n",
    "    - keyword: str, the input keyword to find similar nodes for.\n",
    "    - embeddings: dict, a dictionary where keys are nodes and values are their embeddings.\n",
    "    - embedding_object: HuggingFaceEmbeddings.\n",
    "    - N_samples: int, number of top similar nodes to return.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples [(node, similarity), ...] in descending order of similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the keyword using the embedding endpoint\n",
    "    keyword_embedding = embedding_object.embed_query(keyword)\n",
    "    \n",
    "    # Initialize a min-heap\n",
    "    min_heap = []\n",
    "    heapq.heapify(min_heap)\n",
    "    \n",
    "    for node, embedding in embeddings.items():\n",
    "        embedding = np.array(embedding)  # Ensure embedding is a numpy array\n",
    "        if embedding.ndim > 1:\n",
    "            embedding = embedding.flatten()  # Flatten only if not already 1-D\n",
    "        similarity = 1 - cosine(keyword_embedding, embedding)  # Cosine similarity\n",
    "        \n",
    "        # If the heap is smaller than N_samples, just add the current node and similarity\n",
    "        if len(min_heap) < N_samples:\n",
    "            heapq.heappush(min_heap, (similarity, node))\n",
    "        else:\n",
    "            # If the current similarity is greater than the smallest similarity in the heap\n",
    "            if similarity > min_heap[0][0]:\n",
    "                heapq.heappop(min_heap)  # Remove the smallest\n",
    "                heapq.heappush(min_heap, (similarity, node))  # Add the current node and similarity\n",
    "                \n",
    "    # Convert the min-heap to a sorted list in descending order of similarity\n",
    "    best_nodes = sorted(min_heap, key=lambda x: -x[0])\n",
    "\n",
    "    assert len(best_nodes) >0 , f\"Error in graph_tools.find_best_fitting_node_list(): No nodes found for keyword {keyword}.\"\n",
    "    \n",
    "    # Return a list of tuples (node, similarity)\n",
    "    return [(node, similarity) for similarity, node in best_nodes]\n",
    "\n",
    "#From GraphReasoning_Mod.graph_analysis. Add metadata if available\n",
    "def print_node_pairs_edge_title_metadata(G):\n",
    "    pairs_and_titles = []\n",
    "    for node1, node2, data in G.edges(data=True):\n",
    "        assert isinstance(data, dict), f\"graph_analysis.print_node_pairs_edge_tile(): Expected data to be a dictionary, but got {type(data)}\"\n",
    "        assert 'title' in data, f\"graph_analysis.print_node_pairs_edge_tile(): Expected 'title' key in data, but got {data.keys()}\"\n",
    "\n",
    "        metadata = data.get('metadata', 'No metadata')  # Default to 'No metadata' if not present #TODO: check the .get method\n",
    "\n",
    "        # Assuming 'title' is the edge attribute you want to print\n",
    "        title = data.get('title', 'No title')  # Default to 'No title' if not present\n",
    "        pairs_and_titles.append(f\"{node1}, {title}, {node2} | {metadata}\")\n",
    "    #print (\"Format: node_1, relationship, node_2 | metadata\")\n",
    "    return pairs_and_titles\n",
    "\n",
    "#From GraphReasoning_Mod.graph_analysis\n",
    "def print_path_with_edges_as_list(G, path, keywords_separator=' --> '):\n",
    "    path_elements = []\n",
    "\n",
    "    for i in range(len(path) - 1):\n",
    "        node1 = path[i]\n",
    "        node2 = path[i + 1]\n",
    "\n",
    "        # Retrieve edge data between node1 and node2\n",
    "        edge_data = G.get_edge_data(node1, node2)\n",
    "\n",
    "        # Access the 'title' directly from the edge_data\n",
    "        if edge_data:\n",
    "            edge_title = edge_data.get('title', 'No title')\n",
    "        else:\n",
    "            edge_title = 'No title'\n",
    "\n",
    "        # Construct the path elements, inserting the edge title between node pairs\n",
    "        if i == 0:\n",
    "            path_elements.append(node1)  # Add the first node\n",
    "        path_elements.append(edge_title)  # Add the edge title\n",
    "        path_elements.append(node2)  # Add the second node\n",
    "\n",
    "    # Convert the list of path elements into a string with the specified separator\n",
    "    as_string = keywords_separator.join(path_elements)\n",
    "\n",
    "    return path_elements, as_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define the structure of the GraphRetrievalState dictionary\n",
    "class GraphRAGData(TypedDict):\n",
    "    keyword_1: str\n",
    "    keyword_2: str\n",
    "    graph_path_str: str\n",
    "    node_list: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New GraphRAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the state structure\n",
    "class State_kg(TypedDict):\n",
    "    student_name: str\n",
    "    profile: str\n",
    "    request: str  # The overarching question or request\n",
    "    context: List[GraphRAGData]\n",
    "    answer: str\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template =chatbot_prompt_template,\n",
    "    input_variables= [\"profile\", \"context\", \"request\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_keyphase(llm, input_text, num_return=2):\n",
    "    \"\"\"\n",
    "    This function analyzes the input text and extracts the top N most relevant phrases\n",
    "    based on the text content. \n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text from which to extract keywords.\n",
    "    top_n (int): The number of top phrases to return. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of the top N keyphrases extracted from the text ordered by importance as determined by the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    num_keyphrase_to_generate = num_return\n",
    "\n",
    "    #TODO: consider student current state and desired state as desired output\n",
    "    prompt_keyphrase = f\"\"\"\n",
    "You are an expert in creating concise, high-quality summaries optimized for embedding and retrieval. Your task is to extract key phrases or sentences from a provided text block (delimited by ```) that encapsulate the most important content. These summaries should focus on essential details and exclude any formatting instructions or examples unrelated to the content.\n",
    "\n",
    "### Tasks:\n",
    "1. Ignore all formatting instructions or irrelevant text in the provided text block.\n",
    "2. Generate {num_keyphrase_to_generate} unique and meaningful sentences that align with the request and the associated student profile.\n",
    "3. Ensure the extracted sentences are ordered by their importance to the content of the text block.\n",
    "4. Output the key phrases in a plain list format, with no additional text or commentary.\n",
    "\n",
    "### Example:\n",
    "#### Number of sentences to generate: 3  \n",
    "#### Input Text Block:\n",
    "    ```\n",
    "    You are an expert tutor in mathematics and linear algebra, specializing in personalized and context-aware explanations. Your goal is to provide clear, relevant, and engaging responses tailored to the student’s profile, retrieved context, and their request. Use the information provided to adapt your explanation to their background, strengths, weaknesses, and preferences.\n",
    "\n",
    "### Student Profile:\n",
    "\n",
    "        Background: Graduate student pursuing an Industrial Engineering degree with exposure to optimization techniques.\n",
    "        Strengths: Comfortable with mathematical modeling and programming in Python.\n",
    "        Weaknesses: Lacks practical experience with stochastic and simulation models.\n",
    "        Preferences: Prefers structured lessons with hands-on coding exercises and case studies.\n",
    "        Prior Course History: \n",
    "        - Linear Algebra for Engineers\n",
    "        - Optimization Techniques\n",
    "        - Applied Probability and Statistics\n",
    "\n",
    "### Retrieved Context:\n",
    "\n",
    "\n",
    "### Student Request:\n",
    "Help me understand how SVD is used for dimensionality reduction in machine learning. Provide a Python-based example and resources to deepen my understanding.\n",
    "\n",
    "### Instructions for Response:\n",
    "1. Start with a concise summary addressing the student’s request directly.\n",
    "2. Provide a detailed explanation that aligns with their preferences (e.g., visual aids, practical examples, or technical depth).\n",
    "3. Relate your explanation to the student’s strengths while addressing their weaknesses constructively.\n",
    "4. If applicable, suggest additional resources (e.g., textbooks, videos, or Python libraries) to help the student further explore the topic.\n",
    "5. End with an encouraging note to motivate the student.\n",
    "\n",
    "### Example Structure for Your Response:\n",
    "**1. Summary:** A short, focused answer to the request.  \n",
    "**2. Detailed Explanation:** An in-depth response tailored to the student’s background and the retrieved context.  \n",
    "**3. Additional Resources (if applicable):** Suggestions for further exploration or practice.  \n",
    "**4. Encouragement:** A motivating closing statement.  \n",
    "\n",
    "Remember to always prioritize clarity and ensure the response is aligned with the student’s knowledge level and preferences.\n",
    "    ```\n",
    "\n",
    "    #### Example Output:\n",
    "    [ \n",
    "        Graduate student specializing in Industrial Engineering with a background in optimization techniques and prior coursework in Linear Algebra, Optimization, and Applied Probability. ;\n",
    "        Explanation of SVD for dimensionality reduction in machine learning, including a Python-based example and additional resources for further study. ;\n",
    "        Limited experience with practical applications for simulation models.\n",
    "    ]\n",
    "\n",
    "    ### Instructions:\n",
    "    Using the format and approach outlined above, extract key phrases from the text block provided below:\n",
    "\n",
    "    Number of sentences to generate: {num_keyphrase_to_generate}\n",
    "    Text Block:\n",
    "    ```\n",
    "    {input_text}\n",
    "    ```\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt_keyphrase)\n",
    "    string_keyphrase=extract(response)\n",
    "\n",
    "\n",
    "    start_index = response.find('[')\n",
    "    end_index = response.rfind(']')\n",
    "    string_keyphrase= response[start_index+1 :end_index]\n",
    "    keyphrase = string_keyphrase.split(\";\")\n",
    "    keyphrase = [phrase.strip() for phrase in keyphrase] # Remove leading/trailing whitespace\n",
    "\n",
    "    keyphrase = keyphrase[:num_return]\n",
    "    assert len(keyphrase) == num_return, f\"determine_keyphrase() error. Expected {num_return} keyphrase, but got {len(keyphrase)} keyphrase.\"\n",
    "\n",
    "    return keyphrase\n",
    "\n",
    "# # #usage\n",
    "# keyphrase = determine_keyphase(llm, prompt_test, num_return=4)\n",
    "# print(keyphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_retrieve(g, node_embeddings, embedding_object, input_text, num_keywords=2, total_N_limit=300, out_dir = './outputs/')->List[GraphRAGData]:\n",
    "    \"\"\" \n",
    "    This function will retrieve content from a graph based on the input text.\n",
    "\n",
    "    Retrieved context includes:\n",
    "    - Path between keywords (shortest path with two hops)\n",
    "    - all nodes and edges for all nodes with two tops of the shortest path\n",
    "\n",
    "    Note: Based on GraphReasoning.graph_analysis.find_path_and_reason()\n",
    "    \"\"\"\n",
    "\n",
    "    ###Design decisions for awareness and future adjustment\n",
    "    #keyword pair on combinations\n",
    "    #llm determines most important keyphrase\n",
    "    #limits the number of keyword pairs to 10 becuase of o(n^2) complexity\n",
    "\n",
    "\n",
    "    #determine keyphrase\n",
    "    keyphrase = determine_keyphase(llm, input_text, num_return=num_keywords)\n",
    "\n",
    "    #make set of keyword pairs. Limit to 10 due to O(n^2) complexity\n",
    "    keyphrase_pairs = set()\n",
    "    for i in range(len(keyphrase)):\n",
    "        for j in range(i+1, len(keyphrase)):\n",
    "            keyphrase_pairs.add((keyphrase[i], keyphrase[j]))\n",
    "            if len(keyphrase_pairs) >= 10:\n",
    "                break\n",
    "        if len(keyphrase_pairs) >= 10:\n",
    "            break\n",
    "\n",
    "    # print(f\"keyphrase: {keyphrase}\")#DEBUG\n",
    "    # print(f\"Keyword pairs: {keyword_pairs}\")#DEBUG\n",
    "\n",
    "    #limit the number of nodes for each keyword pair to sum to total_N_limit\n",
    "    N_limit = total_N_limit//len(keyphrase_pairs) if total_N_limit//len(keyphrase_pairs) > 0 else None\n",
    "\n",
    "    #get path and node info from KG for each keyword pair\n",
    "    retrieved_data_list = []\n",
    "    unique_nodes = set() # Set to keep track of unique nodes\n",
    "    for key_1, key_2 in keyphrase_pairs:\n",
    "        #TODO: address the fact that the best fitting node for the keywords is often the same node so it gets repeated and no new information is added. \n",
    "        #TODO: address semantic search to get nodes that are best related to the keywords. e.g. diversify by source, how to add keyword context, etc.\n",
    "\n",
    "        similarity_fit_n1=0\n",
    "        similarity_fit_n2=0 #which path to include 0=only best, 1 onlysecond best, etc.\n",
    "        graph_path_str = \"\"\n",
    "        node_list = []\n",
    "\n",
    "        (best_node_1, best_similarity_1, best_node_2, best_similarity_2), path_of_nodes, path_graph, _=find_path(\n",
    "            g, node_embeddings, embedding_object, keyword_1 =key_1,  keyword_2= key_2,verbatim= False,\n",
    "            similarity_fit_ID_node_1=similarity_fit_n1,similarity_fit_ID_node_2=similarity_fit_n2,\n",
    "            save_files=True, data_dir=out_dir,\n",
    "        )\n",
    "\n",
    "        # print(f\"keyword: {key_1}, best node: {best_node_1}, Node similarity: {best_similarity_1}\")#DEBUG\n",
    "        # print(f\"keyword: {key_2}, best node: {best_node_2}, Node similarity: {best_similarity_2}\")#DEBUG\n",
    "\n",
    "        #if no path between nodes then return string\n",
    "        if path_of_nodes is None or len(path_of_nodes) < 1:\n",
    "            path_list_string = f\"No path found between (keyphrase: {key_1}, best node: {best_node_1}) and (keyphrase: {key_2}, best node: {best_node_2}).\"\n",
    "            #node_list= [\"\"] * N_limit\n",
    "\n",
    "        else:\n",
    "            #node and relationship pairs context\n",
    "            node_list = print_node_pairs_edge_title_metadata(path_graph)\n",
    "            if N_limit != None:\n",
    "                node_list=node_list[:N_limit]\n",
    "            \n",
    "            #path with relationships\n",
    "            if N_limit != None:\n",
    "                path_of_nodes=path_of_nodes[:N_limit]\n",
    "                _, path_list_string= print_path_with_edges_as_list(g, path_of_nodes)\n",
    "\n",
    "                # print(\"path of nodes: \", path_of_nodes) #debug\n",
    "                # print(\"path graph: \", path_graph)#debug\n",
    "\n",
    "        # # Prevent duplicates for entries in the node lists betwen keyword pair lists \n",
    "        # node_list = [node for node in node_list if node not in unique_nodes] #Remove duplicates from node_list\n",
    "        # unique_nodes.update(node_list)\n",
    "\n",
    "        # print(f\"Node list to add: {node_list}\") #DEBUG\n",
    "\n",
    "        #TODO prioritize \"is source document of\" and \"is prerequisite for\" relationships\n",
    "\n",
    "        # print(f\"Path list string to add: {path_list_string}\")#DEBUG\n",
    "\n",
    "        graph_retrieval_data = {\n",
    "            \"keyword_1\": key_1,\n",
    "            \"keyword_2\": key_2,\n",
    "            \"graph_path_str\": path_list_string,\n",
    "            \"node_list\": node_list,\n",
    "        }\n",
    "        retrieved_data_list.append(graph_retrieval_data)\n",
    "    \n",
    "    return retrieved_data_list\n",
    "\n",
    "# # #usage\n",
    "# retrieved_context_obj= graph_retrieve(graph, existing_node_embeddings, embd, prompt_test, num_keywords=3, total_N_limit=10)\n",
    "# for context_obj in retrieved_context_obj:\n",
    "#     print(f\"Retrieved data list: {context_obj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_response_generate(state:State_kg):\n",
    "    \"\"\"\n",
    "    This function generates a response based on the retrieved context from the graph.\n",
    "    The response includes the path between keywords and the nodes and relationships for each node in the path.\n",
    "\n",
    "    Parameters:\n",
    "    state (LearningPlanState_G): A dictionary containing the retrieved state with context for each keyword pair.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function for formatting\n",
    "    join_strings_newline = lambda strings: '\\n'.join(strings)\n",
    "\n",
    "    # Create context string\n",
    "    graph_context = \"### Knowledge Graph Context:\\n\\n\"\n",
    "    \n",
    "    # Loop through each retrieved context object\n",
    "    for idx, context_obj in enumerate(state[\"context\"], start=1):\n",
    "        keyword_1 = context_obj[\"keyword_1\"]\n",
    "        keyword_2 = context_obj[\"keyword_2\"]\n",
    "        graph_path_str = context_obj[\"graph_path_str\"]\n",
    "        node_list = context_obj[\"node_list\"]\n",
    "\n",
    "        # Add context for each keyword pair\n",
    "        graph_context += f\"**{idx}. Keywords:** `{keyword_1}` and `{keyword_2}`\\n\"\n",
    "        graph_context += f\"- **Path:** {graph_path_str}\\n\"\n",
    "        graph_context += f\"- **Subgraph Relationships:**\\n\"\n",
    "        graph_context += f\"  {join_strings_newline(node_list)}\\n\\n\"\n",
    "\n",
    "    #format the prompt\n",
    "    formatted_prompt = custom_prompt.format(\n",
    "        profile=state[\"profile\"],\n",
    "        request=state[\"request\"],\n",
    "        context=graph_context\n",
    "    )\n",
    "\n",
    "    # print(formatted_prompt)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    state[\"answer\"] = response\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Usage and Testing\n",
    "\n",
    "# state = State_kg(\n",
    "#         profile=student_1[\"profile\"],\n",
    "#         request= student_1[\"requests\"][0],\n",
    "#         context=retrieved_context_obj, \n",
    "#         answer=\"\"\n",
    "#     )\n",
    "\n",
    "# custom_prompt = PromptTemplate(\n",
    "#     template =chatbot_prompt_template,\n",
    "#     input_variables= [\"profile\", \"request\", \"context\"],\n",
    "# )\n",
    "\n",
    "# response = graph_response_generate(state)\n",
    "# print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of nodes: 3484\n",
      "Initial number of edges: 9841\n",
      "Cleaned number of nodes: 3483\n",
      "Cleaned number of edges: 8924\n",
      "Final node data: [('C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', {'group': 1, 'color': '#57dbc2', 'size': 2497})]\n",
      "Final edge data: [('C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'linear algebra', {'title': 'is source document of', 'weight': 1.0})]\n",
      "Number of node embeddings: 3483\n",
      "\n",
      "Initial number of nodes: 77039\n",
      "Initial number of edges: 62284\n",
      "Cleaned number of nodes: 77032\n",
      "Cleaned number of edges: 62257\n",
      "Final node data: [('cartesian product of vector spaces', {'group': 1, 'color': '#57db89', 'size': 1})]\n",
      "Final edge data: [('cartesian product of vector spaces', 'ordered pairs', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)'}\", 'weight': 1.0})]\n",
      "Number of node embeddings: 77032\n",
      "\n",
      "Initial number of nodes: 37709\n",
      "Initial number of edges: 58504\n",
      "Cleaned number of nodes: 37706\n",
      "Cleaned number of edges: 58465\n",
      "Final node data: [('product of transformation matrices', {'group': 891, 'color': '#dbcc57', 'size': 1})]\n",
      "Final edge data: [('product of transformation matrices', 'euclidean lines', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)'}\", 'weight': 1.0})]\n",
      "Number of node embeddings: 37706\n",
      "\n",
      "Initial number of nodes: 18933\n",
      "Initial number of edges: 33153\n",
      "Cleaned number of nodes: 18930\n",
      "Cleaned number of edges: 33124\n",
      "Final node data: [('product of transformation matrices', {'group': 891, 'color': '#dbcc57', 'size': 1})]\n",
      "Final edge data: [('product of transformation matrices', 'euclidean lines', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)'}\", 'weight': 1.0})]\n",
      "Number of node embeddings: 18930\n",
      "\n",
      "Initial number of nodes: 13596\n",
      "Initial number of edges: 24305\n",
      "Cleaned number of nodes: 13593\n",
      "Cleaned number of edges: 24281\n",
      "Final node data: [('product of transformation matrices', {'group': 891, 'color': '#dbcc57', 'size': 1})]\n",
      "Final edge data: [('product of transformation matrices', 'euclidean lines', {'title': 'consists of', 'metadata': \"{'source': 'Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf - page: 3', 'source_type': 'Textbook_PDF', 'title': 'Linear Algebra and its Applications (4th ed.)'}\", 'weight': 1.0})]\n",
      "Number of node embeddings: 13593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for graph_name in graph_dict.keys():\n",
    "    \n",
    "    graph=graph_dict[graph_name]\n",
    "    embeddings = embds_dict[graph_name]\n",
    "\n",
    "    graph_dict[graph_name], embds_dict[graph_name] = clean_graph(graph,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_pickle_path = \"output_data_records_graph.pkl\"\n",
    "# data_records_graph = []\n",
    "\n",
    "# custom_prompt = PromptTemplate(\n",
    "#     template =chatbot_prompt_template,\n",
    "#     input_variables= [\"profile\", \"request\", \"context\"],\n",
    "# )\n",
    "\n",
    "# for graph_name in tqdm(graph_dict.keys(), desc=\"Generating from Graphs\", total=len(graph_dict)):\n",
    "\n",
    "#     graph=graph_dict[graph_name]\n",
    "#     existing_node_embeddings = embds_dict[graph_name]\n",
    "\n",
    "#     for student_index, student in enumerate(students):\n",
    "#         for request_index, request in enumerate(student[\"requests\"]):\n",
    "\n",
    "#             test_case_id = f\"s{student_index}r{request_index}\"\n",
    "\n",
    "#             request = student[\"requests\"][request_index]\n",
    "\n",
    "#             empty_context = {\n",
    "#                 \"keyword_1\": \"n/a\",\n",
    "#                 \"keyword_2\": \"n/a\",\n",
    "#                 \"graph_path_str\": \"\",\n",
    "#                 \"node_list\": [\"\"],\n",
    "#             }\n",
    "#             # Initialize the state\n",
    "#             state = State_kg(\n",
    "#                 profile=student_1[\"profile\"],\n",
    "#                 request= request,\n",
    "#                 context= [empty_context], \n",
    "#                 answer=\"\"\n",
    "#             )\n",
    "\n",
    "#             #format the prompt\n",
    "#             formatted_prompt = custom_prompt.format(\n",
    "#                 profile=state[\"profile\"],\n",
    "#                 request=state[\"request\"],\n",
    "#                 context=state[\"context\"]\n",
    "#             )\n",
    "\n",
    "#             try:\n",
    "#                 out_dir = f\"./outputs/{graph_name}/\"\n",
    "#                 retrieved_context_obj= graph_retrieve(graph, existing_node_embeddings, embd, formatted_prompt, num_keywords=3, total_N_limit=100, out_dir=out_dir)\n",
    "\n",
    "#                 # print(f\"Retrieved context for {student['test_case']}\") #DEBUG\n",
    "\n",
    "#                 # update the state with the retrieved context\n",
    "#                 state[\"context\"] = retrieved_context_obj\n",
    "\n",
    "#                 try:\n",
    "#                     # Execute the workflow\n",
    "#                     state = graph_response_generate(state)\n",
    "#                 except Exception as e:\n",
    "#                     state[\"answer\"] = f\"Error in graph response generation for {student['test_case']}: {e}\"\n",
    "#                     print(f\"Error in graph response generation for {student['test_case']}: {e}\")\n",
    "#                     break\n",
    "\n",
    "            \n",
    "#             except Exception as e:\n",
    "#                 state[\"answer\"] = f\"Error in graph retrieval for {student['test_case']} and graph {graph_name}: {e}\"\n",
    "            \n",
    "#             # Format LLM context as a **string**\n",
    "#             llm_context = \"\\n\\n\".join(\n",
    "#                 f\"Keywords: {context_obj.get('keyword_1', 'n/a')} and {context_obj.get('keyword_2', 'n/a')}\\n\"\n",
    "#                 f\"Path: {context_obj.get('graph_path_str', '')}\\n\"\n",
    "#                 f\"Node List: {', '.join(context_obj.get('node_list', ['']))}\"\n",
    "#                 for context_obj in state[\"context\"]\n",
    "#             )\n",
    "\n",
    "#             # Append data to the list\n",
    "#             data_records_graph.append({\n",
    "#                 \"index\": len(data_records_graph),  # Auto-incrementing index\n",
    "#                 \"type\": graph_name,\n",
    "#                 \"test_case\": test_case_id,\n",
    "#                 \"student\": student[\"profile\"],\n",
    "#                 \"request\": request,\n",
    "#                 \"context\": llm_context,\n",
    "#                 \"response\": state['answer']\n",
    "#             })\n",
    "\n",
    "#     try:\n",
    "#         with open(output_pickle_path, \"wb\") as f:\n",
    "#             pickle.dump(data_records_graph, f)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving output pickle file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regenerate a specific test case\n",
    "\n",
    "output_pickle_path = \"output_data_records_graph.pkl\"\n",
    "data_records_graph = []\n",
    "\n",
    "for graph_name in ['GR_kg_no_refine3_0.85', 'GR_kg_no_refine3_0.75']:\n",
    "    graph=graph_dict[graph_name]\n",
    "    existing_node_embeddings = embds_dict[graph_name]\n",
    "\n",
    "    custom_prompt = PromptTemplate(\n",
    "        template =chatbot_prompt_template,\n",
    "        input_variables= [\"profile\", \"request\", \"context\"],\n",
    "    )\n",
    "    student_index = 1\n",
    "    request_index = 0\n",
    "\n",
    "\n",
    "    test_case_id = f\"s{student_index}r{request_index}\"\n",
    "\n",
    "    student = students[student_index]\n",
    "    request = student[\"requests\"][request_index]\n",
    "\n",
    "    empty_context = {\n",
    "        \"keyword_1\": \"n/a\",\n",
    "        \"keyword_2\": \"n/a\",\n",
    "        \"graph_path_str\": \"\",\n",
    "        \"node_list\": [\"\"],\n",
    "    }\n",
    "    # Initialize the state\n",
    "    state = State_kg(\n",
    "        profile=student_1[\"profile\"],\n",
    "        request= request,\n",
    "        context= [empty_context], \n",
    "        answer=\"\"\n",
    "    )\n",
    "\n",
    "    #format the prompt\n",
    "    formatted_prompt = custom_prompt.format(\n",
    "        profile=state[\"profile\"],\n",
    "        request=state[\"request\"],\n",
    "        context=state[\"context\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        out_dir = f\"./outputs/{graph_name}/\"\n",
    "        retrieved_context_obj= graph_retrieve(graph, existing_node_embeddings, embd, formatted_prompt, num_keywords=3, total_N_limit=100, out_dir=out_dir)\n",
    "\n",
    "        # print(f\"Retrieved context for {student['test_case']}\") #DEBUG\n",
    "\n",
    "        # update the state with the retrieved context\n",
    "        state[\"context\"] = retrieved_context_obj\n",
    "\n",
    "        try:\n",
    "            # Execute the workflow\n",
    "            state = graph_response_generate(state)\n",
    "        except Exception as e:\n",
    "            state[\"answer\"] = f\"Error in graph response generation for {student['test_case']}: {e}\"\n",
    "            print(f\"Error in graph response generation for {student['test_case']}: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        state[\"answer\"] = f\"Error in graph retrieval for {student['test_case']} and graph {graph_name}: {e}\"\n",
    "\n",
    "    # Format LLM context as a **string**\n",
    "    llm_context = \"\\n\\n\".join(\n",
    "        f\"Keywords: {context_obj.get('keyword_1', 'n/a')} and {context_obj.get('keyword_2', 'n/a')}\\n\"\n",
    "        f\"Path: {context_obj.get('graph_path_str', '')}\\n\"\n",
    "        f\"Node List: {', '.join(context_obj.get('node_list', ['']))}\"\n",
    "        for context_obj in state[\"context\"]\n",
    "    )\n",
    "\n",
    "    # Append data to the list\n",
    "    data_records_graph.append({\n",
    "        \"index\": len(data_records_graph),  # Auto-incrementing index\n",
    "        \"type\": graph_name,\n",
    "        \"test_case\": test_case_id,\n",
    "        \"student\": student[\"profile\"],\n",
    "        \"request\": request,\n",
    "        \"context\": llm_context,\n",
    "        \"response\": state['answer']\n",
    "    })\n",
    "\n",
    "try:\n",
    "    with open(output_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(data_records_graph, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output pickle file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---->response: \n",
      "---\n",
      "\n",
      "**1. Summary:**\n",
      "Eigenvalues of a positive definite matrix are all positive, which is a key characteristic that helps define positive definiteness. We'll explore this relationship using a Python-based example to illustrate the concept.\n",
      "\n",
      "**2. Detailed Explanation:**\n",
      "\n",
      "Positive definite matrices play a crucial role in various fields, including optimization, machine learning, and statistics. To understand how eigenvalues relate to positive definite matrices, let's first recall the definition of a positive definite matrix:\n",
      "\n",
      "A symmetric matrix A is positive definite if, for any non-zero vector v, the inequality v^T * A * v > 0 holds true.\n",
      "\n",
      "Now, let's consider the relationship between eigenvalues and positive definite matrices. The eigenvalues of a positive definite matrix A are all real and positive. This is because the quadratic form v^T * A * v can be expressed as a sum of squares of the eigenvalues, each multiplied by the corresponding eigenvector component. Since the eigenvalues are positive, the entire expression is also positive, satisfying the condition for positive definiteness.\n",
      "\n",
      "To illustrate this relationship, let's consider a simple 2x2 positive definite matrix and compute its eigenvalues using Python and NumPy:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Define a 2x2 positive definite matrix\n",
      "A = np.array([[3, 2], [2, 2]])\n",
      "\n",
      "# Compute the eigenvalues\n",
      "eigenvalues, _ = np.linalg.eig(A)\n",
      "\n",
      "print(\"Eigenvalues:\", eigenvalues)\n",
      "```\n",
      "\n",
      "When you run this code, you'll find that the eigenvalues are both positive, which is consistent with our earlier discussion. This Python-based example helps visualize the relationship between eigenvalues and positive definite matrices.\n",
      "\n",
      "**3. Specific Resources:**\n",
      "\n",
      "To further explore the relationship between eigenvalues and positive definite matrices, you can refer to the following resources from the retrieved context:\n",
      "\n",
      "- **Textbook PDF:** Gilbert Strang's \"Linear Algebra and Its Applications\" (4th ed.) - Chapter 7, Section 3: Positive Definite Matrices (Page 281)\n",
      "  - Link: [Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf](https://www.gilbertstrang.com/lineal/lineal.pdf)\n",
      "\n",
      "- **YouTube Video:** 3Blue1Brown - \"Eigenfaces\" (Eigenvectors and Eigenfaces explained visually)\n",
      "  - Link: [https://www.youtube.com/watch?v=M5pWmSY7lZc](https://www.youtube.com/watch?v=M5pWmSY7lZc)\n",
      "\n",
      "These resources provide a solid foundation for understanding the relationship between eigenvalues and positive definite matrices, with a focus on visualizations and real-world applications – aspects that align with your learning preferences.\n",
      "---->response: \n",
      "```python\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Define a 2x2 positive definite matrix\n",
      "A = np.array([[3, 2], [2, 2]])\n",
      "\n",
      "# Calculate the eigenvalues of the matrix\n",
      "eigenvalues, _ = np.linalg.eig(A)\n",
      "\n",
      "# Check if the eigenvalues are positive\n",
      "is_positive_definite = np.all(eigenvalues > 0)\n",
      "\n",
      "# Print the result\n",
      "print(f\"The matrix is {'not ' if not is_positive_definite else ''}positive definite.\")\n",
      "\n",
      "# Visualize the matrix\n",
      "plt.matshow(A, cmap='hot')\n",
      "plt.colorbar()\n",
      "plt.title('Matrix A')\n",
      "plt.show()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "for record in data_records_graph:\n",
    "    print(f\"---->response: \\n{record['response']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_ilp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
