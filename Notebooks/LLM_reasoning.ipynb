{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation comparison\n",
    "This notebook compares text generation with different augmentation approaches. \n",
    "- Outputs results to `../data/exp_outputs/` for examination.\n",
    "- Two prompt formats examined for generating individualized learning plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO for this notebook:\n",
    "- intermediate test case doesnt generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "root_path = \"c:\\\\Users\\\\jonathan.kasprisin\\\\gitlab\\\\DNoK_GraphRAG\"\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path)\n",
    "\n",
    "output_path = \"./data/exp_outputs/llm_reasoning_output_v2.txt\"\n",
    "context_review_path = \"./data/exp_outputs/llm_reasoning_context_review_v2.txt\"\n",
    "\n",
    "output_pickle_path = \"llm_reason_output_data.pkl\"\n",
    "\n",
    "# Initialize an empty list to collect rows for the DataFrame\n",
    "data_records = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HuggingFace is a company that provides AI models and tools.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "#Initialize the model endpoint\n",
    "HOST_URL_INF = \":8080\"\n",
    "MAX_NEW_TOKENS = 1800\n",
    "\n",
    "TEMPERATURE = 0.2\n",
    "TIMEOUT = 180\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=HOST_URL_INF,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature = TEMPERATURE,\n",
    "    timeout=TIMEOUT,\n",
    ")\n",
    "print(llm.invoke(\"In 10 words, what is HuggingFace?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert tutor in mathematics and linear algebra, specializing in personalized and context-aware explanations. \n",
      "Your goal is to provide clear, relevant, and engaging responses tailored to the student’s profile, retrieved context, and their request. \n",
      "Use the information provided to adapt your explanation to their background, strengths, weaknesses, and preferences.\n",
      "\n",
      "### Student Profile:\n",
      "\n",
      "        Background: Recent college graduate with a degree in Business Administration.\n",
      "        Strengths: Strong organizational and project management skills.\n",
      "        Weaknesses: Limited mathematical background; no prior programming experience.\n",
      "        Preferences: Prefers real-world applications, interactive learning, and visualizations.\n",
      "        Prior Course History: \n",
      "        - Introduction to Business Mathematics\n",
      "        - Basic Statistics for Managers\n",
      "    \n",
      "\n",
      "### Retrieved Context:\n",
      "\n",
      "\n",
      "### Student Request:\n",
      "Help me understand how eigenvalues relate to matrix transformations. Provide content that visually explains this concept and its applications in data analysis.\n",
      "\n",
      "### Instructions for Response:\n",
      "1. Start with a concise text summary addressing the student’s request directly.\n",
      "2. Provide a detailed explanation that aligns with their preferences (e.g., visual aids, practical examples, or technical depth).\n",
      "3. Relate your explanation to the student’s strengths while addressing their weaknesses constructively.\n",
      "4. Suggest specific content from the retrieved context to help the student further examine relationships between the topics.\n",
      "\n",
      "### Example Structure for Your Response:\n",
      "**1. Summary:** A short, focused answer to the request.  \n",
      "**2. Detailed Explanation:** An in-depth response tailored to the student’s background and the retrieved context.  \n",
      "**3. Specific Resources:** Specifics of where to access material that supports the detailed explanation.  \n",
      "  \n",
      "\n",
      "Remember to always prioritize clarity and ensure the response is aligned with the student’s knowledge level and preferences.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load prompt templates and testing student profiles\n",
    "from utils.prompt_templates import chatbot_prompt_template\n",
    "from utils.test_case_data   import student_1, student_2, student_3\n",
    "\n",
    "students = [student_1, student_2, student_3]\n",
    "\n",
    "#test prompt template\n",
    "prompt_test = chatbot_prompt_template.format(\n",
    "        # student_name=student_1[\"name\"],\n",
    "        profile=student_1[\"profile\"],\n",
    "        context=\"\",\n",
    "        request=student_1[\"requests\"][0],\n",
    "    )\n",
    "\n",
    "print(prompt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Test \n",
    "Mistral-NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**1. Summary:**\n",
      "Eigenvalues and eigenvectors are special features of a matrix that describe how it transforms a vector. They are crucial in data analysis as they help identify patterns and reduce dimensionality. Visualizing these concepts with real-world examples will make them more accessible.\n",
      "\n",
      "**2. Detailed Explanation:**\n",
      "\n",
      "Imagine you're playing a game where you're given a matrix (a set of rules) that transforms vectors (your starting point). Some starting points (eigenvectors) remain unchanged or scaled (eigenvalues) when you apply the matrix rules. These are the 'eigen' (self) features of the matrix.\n",
      "\n",
      "Let's consider a simple 2x2 matrix A:\n",
      "```\n",
      "A = [3 1;\n",
      "     2 2]\n",
      "```\n",
      "If you start at point (1, 0), applying A moves you to (5, 2). But if you start at (1, 1), applying A keeps you at (1, 1) - this is an eigenvector with eigenvalue 1.\n",
      "\n",
      "In data analysis, eigenvalues and eigenvectors help in Principal Component Analysis (PCA), a technique used to reduce dimensionality while retaining as much information as possible. Here's a visual example:\n",
      "\n",
      "![PCA Example](https://i.imgur.com/X4Z94jS.png)\n",
      "(Source: [Towards Data Science](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60))\n",
      "\n",
      "In this example, the first principal component (eigenvector with the largest eigenvalue) captures the most variance in the data, while the second captures the remaining variance. This helps us visualize and understand complex data in a simpler, lower-dimensional space.\n",
      "\n",
      "**3. Specific Resources:**\n",
      "\n",
      "- To learn more about eigenvalues and eigenvectors, watch this interactive video: [Khan Academy - Eigenvectors and eigenvalues](https://www.khanacademy.org/math/linear-algebra/core-topics/eigenvectors-and-eigenvalues/a/what-are-eigenvectors-and-eigenvalues)\n",
      "- For a practical application of PCA, follow this step-by-step guide using Python and scikit-learn: [PCA using Python and scikit-learn](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "response = llm.invoke(prompt_test)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for student_index, student in enumerate(students):\n",
    "    for request_index, request in enumerate(student[\"requests\"]):\n",
    "        # Generate test_case ID using student index and request index\n",
    "        test_case_id = f\"s{student_index}r{request_index}\"\n",
    "\n",
    "        request = student[\"requests\"][request_index]\n",
    "        # Fill in the template\n",
    "        prompt = chatbot_prompt_template.format(\n",
    "            profile=student[\"profile\"],\n",
    "            context=\"\",\n",
    "            request=request,\n",
    "        )\n",
    "    \n",
    "        #add response to output text file\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Append data to the list\n",
    "        data_records.append({\n",
    "            \"index\": len(data_records),  # Auto-incrementing index\n",
    "            \"type\": \"Basic\",\n",
    "            \"test_case\": test_case_id,\n",
    "            \"student\": student[\"profile\"],\n",
    "            \"request\": request,\n",
    "            \"context\": \"\",\n",
    "            \"response\": response\n",
    "        })\n",
    "\n",
    "try:\n",
    "    with open(output_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(data_records, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output pickle file: {e}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"dunzhang/stella_en_1.5B_v5\" #\"BAAI/bge-small-en-v1.5\" #dunzhang/stella_en_1.5B_v5\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embd = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "\n",
    "vector_store_path = \"./data/storage/chroma_db_stella_1.5B_chunk150\" #\"../data/storage/chroma_db_stella_1.5B\"\n",
    "\n",
    "                               \n",
    "vector_store = Chroma(\n",
    "    embedding_function=embd,\n",
    "    persist_directory=vector_store_path,\n",
    "    collection_name= \"full_vstore_stella1.5B_chunk150\", #\"full_vstore_stella1.5B\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the vector store: 124353\n",
      "Preview of the first document:\n",
      "{'ids': ['10665c28-7898-4d71-8e79-fe332f7a89eb'], 'embeddings': array([], dtype=float64), 'documents': ['Linear Algebra and Its Applications\\nFourth Edition\\nGilbert Strang\\ny\\nx y z \\x1e \\x0c \\nz\\nAx b\\x1e\\nb\\n0\\nAy b\\x1e\\n0Az \\x1e\\n0'], 'uris': None, 'data': None, 'metadatas': [{'page': 1, 'source': 'C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'start_index': 0}], 'included': [<IncludeEnum.embeddings: 'embeddings'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "# Get the number of documents in the vector store\n",
    "num_documents = vector_store._collection.count()\n",
    "\n",
    "# Print the number of documents\n",
    "print(f\"Number of documents in the vector store: {num_documents}\")\n",
    "\n",
    "# Preview one of the documents (assuming the documents are stored in a collection)\n",
    "if num_documents > 0:\n",
    "    # Retrieve the first document (or any document by its ID)\n",
    "    document = vector_store._collection.peek(limit=1)\n",
    "    \n",
    "    # Print a preview of the document\n",
    "    print(\"Preview of the first document:\")\n",
    "    print(document)\n",
    "else:\n",
    "    print(\"No documents found in the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the state structure\n",
    "class LearningPlanState(TypedDict):\n",
    "    profile: str\n",
    "    request: str  # The overarching question or request\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: LearningPlanState, k:int =20):\n",
    "    # Combine the student's profile and objectives to form the query\n",
    "    query = f\"Student Profile: {state['profile']}\\nRequest: {state['request']}\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=k)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state:LearningPlanState):\n",
    "    #get the relevant documents and source metadata\n",
    "    docs_content = \"\\n\\n\".join(\n",
    "        f\"Content source: {doc.metadata.get('source', 'Unknown')}\\n Content: {doc.page_content}\"\n",
    "        for doc in state[\"context\"]\n",
    "    )\n",
    "    formatted_prompt =  chatbot_prompt_template.format(\n",
    "        profile=state['profile'],\n",
    "        context=docs_content,\n",
    "        request=state['request']\n",
    "    )\n",
    "    #print(formatted_prompt)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "# Build the workflow #fix\n",
    "workflow_builder = StateGraph(LearningPlanState).add_sequence([retrieve, generate])\n",
    "workflow_builder.add_edge(START, \"retrieve\")\n",
    "workflow = workflow_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**1. Summary:**\n",
      "Eigenvalues and eigenvectors are s\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "state = LearningPlanState(\n",
    "        profile=student_1[\"profile\"],\n",
    "        request=student_1[\"requests\"][0],\n",
    "        context=[],\n",
    "        answer=\"\"\n",
    "    )\n",
    "\n",
    "response = workflow.invoke(state)\n",
    "print(response[\"answer\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for student_index, student in enumerate(students):\n",
    "    for request_index, request in enumerate(student[\"requests\"]):\n",
    "        # Generate test_case ID using student index and request index\n",
    "        test_case_id = f\"s{student_index}r{request_index}\"\n",
    "\n",
    "        request = student[\"requests\"][request_index]\n",
    "\n",
    "        # Initialize the state\n",
    "        state = LearningPlanState(\n",
    "            profile=student[\"profile\"],\n",
    "            request=request,\n",
    "            context=[],\n",
    "            answer=\"\"\n",
    "        )\n",
    "        \n",
    "        # Execute the workflow\n",
    "        response = workflow.invoke(state)\n",
    "\n",
    "        #Write context to review file in format the llm sees\n",
    "        llm_context = \"\\n\\n\".join(\n",
    "            f\"Content source: {doc.metadata.get('source', 'Unknown')}\\n Content: {doc.page_content}\"\n",
    "            for doc in response['context']\n",
    "        )\n",
    "\n",
    "        # Append data to the list\n",
    "        data_records.append({\n",
    "            \"index\": len(data_records),  # Auto-incrementing index\n",
    "            \"type\": \"RAG_150chunk\",\n",
    "            \"test_case\": test_case_id,\n",
    "            \"student\": student[\"profile\"],\n",
    "            \"request\": request,\n",
    "            \"context\": llm_context,\n",
    "            \"response\": response[\"answer\"]\n",
    "        })\n",
    "    \n",
    "try:\n",
    "    with open(output_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(data_records, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output pickle file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG w/ larger chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the vector store: 11564\n",
      "Preview of the first document:\n",
      "{'ids': ['62950633-20ca-4022-ab4e-db4789aeee34'], 'embeddings': array([[-0.0374994 , -0.00979656,  0.05323608, ..., -0.02966512,\n",
      "        -0.00455778,  0.03899739]]), 'documents': ['Linear Algebra and Its Applications\\nFourth Edition\\nGilbert Strang\\ny\\nx y z \\x1e \\x0c \\nz\\nAx b\\x1e\\nb\\n0\\nAy b\\x1e\\n0Az \\x1e\\n0'], 'uris': None, 'data': None, 'metadatas': [{'page': 1, 'source': 'C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'start_index': 0}], 'included': [<IncludeEnum.embeddings: 'embeddings'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "\n",
    "vector_store_path = \"./data/storage/chroma_db_stella_1.5B\"\n",
    "\n",
    "                               \n",
    "vector_store = Chroma(\n",
    "    embedding_function=embd,\n",
    "    persist_directory=vector_store_path,\n",
    "    collection_name= \"full_vstore_stella1.5B\",\n",
    ")\n",
    "\n",
    "# Get the number of documents in the vector store\n",
    "num_documents = vector_store._collection.count()\n",
    "\n",
    "# Print the number of documents\n",
    "print(f\"Number of documents in the vector store: {num_documents}\")\n",
    "\n",
    "# Preview one of the documents (assuming the documents are stored in a collection)\n",
    "if num_documents > 0:\n",
    "    # Retrieve the first document (or any document by its ID)\n",
    "    document = vector_store._collection.peek(limit=1)\n",
    "    \n",
    "    # Print a preview of the document\n",
    "    print(\"Preview of the first document:\")\n",
    "    print(document)\n",
    "else:\n",
    "    print(\"No documents found in the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow\n",
    "workflow_builder = StateGraph(LearningPlanState)\n",
    "workflow_builder.add_node(\"retrieve\", lambda state: retrieve(state, k=5))\n",
    "workflow_builder.add_node(\"generate\", generate)\n",
    "workflow_builder.add_edge(START, \"retrieve\")\n",
    "workflow_builder.add_edge(\"retrieve\", \"generate\")\n",
    "workflow = workflow_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**1. Summary:**\n",
      "Eigenvalues and eigenvectors help \n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "state = LearningPlanState(\n",
    "        profile=student_1[\"profile\"],\n",
    "        request=student_1[\"requests\"][0],\n",
    "        context=[],\n",
    "        answer=\"\"\n",
    "    )\n",
    "\n",
    "response = workflow.invoke(state)\n",
    "print(response[\"answer\"][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for student_index, student in enumerate(students):\n",
    "    for request_index, request in enumerate(student[\"requests\"]):\n",
    "        # Generate test_case ID using student index and request index\n",
    "        test_case_id = f\"s{student_index}r{request_index}\"\n",
    "\n",
    "        request = student[\"requests\"][request_index]\n",
    "\n",
    "        # Initialize the state\n",
    "        state = LearningPlanState(\n",
    "            profile=student[\"profile\"],\n",
    "            request=request,\n",
    "            context=[],\n",
    "            answer=\"\"\n",
    "        )\n",
    "        \n",
    "        # Execute the workflow\n",
    "        response = workflow.invoke(state)\n",
    "\n",
    "        #Write context to review file in format the llm sees\n",
    "        llm_context = \"\\n\\n\".join(\n",
    "            f\"Content source: {doc.metadata.get('source', 'Unknown')}\\n Content: {doc.page_content}\"\n",
    "            for doc in response['context']\n",
    "        )\n",
    "\n",
    "        # Append data to the list\n",
    "        data_records.append({\n",
    "            \"index\": len(data_records),  # Auto-incrementing index\n",
    "            \"type\": \"RAG_1500chunk\",\n",
    "            \"test_case\": test_case_id,\n",
    "            \"student\": student[\"profile\"],\n",
    "            \"request\": request,\n",
    "            \"context\": llm_context,\n",
    "            \"response\": response[\"answer\"]\n",
    "        })\n",
    "\n",
    "try:\n",
    "    with open(output_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(data_records, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output pickle file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load different KGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GR_kg_no_refine_graph_path = \"../data/generated_graphs/GR_no_refine/final_augmented_graph.graphml\"\n",
    "GR_kg_w_refine_graph_path = \"../data/generated_graphs/GR_w_refine/final_augmented_graph.graphml\"\n",
    "langchain_kg_graph_path = \"../data/generated_graphs/langchain_KG/langchain_full_kg.graphml\"\n",
    "sme_kg_graph_path = \"../data/generated_graphs/SME_graph/DNoKv3.graphml\"\n",
    "\n",
    "#dictionary of graphs\n",
    "graphs_paths = {\n",
    "    \"GR_kg_no_refine\": GR_kg_no_refine_graph_path,\n",
    "    \"GR_kg_w_refine\": GR_kg_w_refine_graph_path,\n",
    "    \"langchain_kg\": langchain_kg_graph_path,\n",
    "    \"sme_kg\": sme_kg_graph_path\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load graph GR_kg_no_refine from ../data/generated_graphs/GR_no_refine/final_augmented_graph.graphml: [Errno 2] No such file or directory: '../data/generated_graphs/GR_no_refine/final_augmented_graph.graphml'\n",
      "\n",
      "Failed to load graph GR_kg_w_refine from ../data/generated_graphs/GR_w_refine/final_augmented_graph.graphml: [Errno 2] No such file or directory: '../data/generated_graphs/GR_w_refine/final_augmented_graph.graphml'\n",
      "\n",
      "Failed to load graph langchain_kg from ../data/generated_graphs/langchain_KG/langchain_full_kg.graphml: [Errno 2] No such file or directory: '../data/generated_graphs/langchain_KG/langchain_full_kg.graphml'\n",
      "\n",
      "Failed to load graph sme_kg from ../data/generated_graphs/SME_graph/DNoKv3.graphml: [Errno 2] No such file or directory: '../data/generated_graphs/SME_graph/DNoKv3.graphml'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import traceback\n",
    "\n",
    "graph_dict = {}\n",
    "\n",
    "for graph_name, graph_path in graphs_paths.items():\n",
    "\n",
    "    try:\n",
    "        graph= nx.read_graphml(graph_path)\n",
    "        graph_dict[graph_name] = graph\n",
    "\n",
    "        # Print info about the graph\n",
    "        print(f\"Loaded Graph: {graph_name}\")\n",
    "        print(f\"-->Number of nodes: {graph.number_of_nodes()}, Number of edges: {graph.number_of_edges()}\")\n",
    "        print(f\"-->example node data: {list(graph.nodes(data=True))[:1]}\")\n",
    "        print(f\"-->example edge data: {list(graph.edges(data=True))[:1]}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load graph {graph_name} from {graph_path}: {e}\\n\")\n",
    "        #traceback.print_exc()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/generated_graphs/langchain_KG/full_graph_documents.pkl '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnx graph built with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_nxe\u001b[38;5;241m.\u001b[39mget_number_of_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph_nxe\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/generated_graphs/langchain_KG/full_graph_documents.pkl \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     22\u001b[0m     graph_documents \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     24\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/generated_graphs/langchain_KG/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#save dropped docs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jonathan.kasprisin\\AppData\\Local\\miniconda3\\envs\\graphrag\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/generated_graphs/langchain_KG/full_graph_documents.pkl '"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "#load langchain graph directly as a NetworkxEntityGraph\n",
    "\n",
    "def make_nxe_graph(graph_documents) -> nx.Graph:\n",
    "    print(f\"Making nx graph from {len(graph_documents)} graph documents\")\n",
    "    graph_nxe = NetworkxEntityGraph()\n",
    "    for doc in graph_documents:\n",
    "        try:\n",
    "            for node in doc.nodes:\n",
    "                graph_nxe.add_node(node.id)\n",
    "            for edge in doc.relationships:\n",
    "                graph_nxe._graph.add_edge(edge.source.id, edge.target.id, relation=edge.type)\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document to nx graph: {doc.source.metadata}, {e}\")\n",
    "    print(f\"nx graph built with {graph_nxe.get_number_of_nodes()} nodes.\") \n",
    "    return graph_nxe\n",
    "\n",
    "with open(\"../data/generated_graphs/langchain_KG/full_graph_documents.pkl \", \"rb\") as file:\n",
    "    graph_documents = pickle.load(file)\n",
    "\n",
    "save_dir = \"../data/generated_graphs/langchain_KG/\" #save dropped docs\n",
    "\n",
    "filtered_graph_documents = []\n",
    "for doc in graph_documents:\n",
    "    valid_nodes = [node for node in doc.nodes if node.type]\n",
    "    if valid_nodes:\n",
    "        doc.nodes = valid_nodes\n",
    "        filtered_graph_documents.append(doc)\n",
    "    else:\n",
    "        with open(save_dir+\"dropped_docs.txt\", \"a\") as f:\n",
    "            f.write(f\" 'Dropped doc.metadata': '{doc.source.metadata}'\\n\")\n",
    "\n",
    "langchain_nxe_graph = make_nxe_graph(filtered_graph_documents)\n",
    "\n",
    "# Print info about the graph\n",
    "# Try to get node data (adjust method name as needed)\n",
    "try:\n",
    "    triples = langchain_nxe_graph.get_triples()\n",
    "    print(f\"Example triple: {triples[0]}\")\n",
    "    entity= triples[0][0]\n",
    "    print(f\"Example node: {entity}\")\n",
    "    knowledge = langchain_nxe_graph.get_entity_knowledge(entity, 3)\n",
    "    print(f\"Example node knowledge: {knowledge}\")\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Unable to access node data. Check the class documentation for the correct method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save networkentity graph as graphml\n",
    "nx.write_graphml(langchain_nxe_graph._graph, \"../data/generated_graphs/langchain_KG/langchain_nxe_graph.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG - Langchain\n",
    "Doesnt work for other KG, \n",
    "_Note: Does not find any context with langchain library for GraphQA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restructure KG to work with langchain NetworkxEntityGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.networkx_graph import KnowledgeTriple\n",
    "# Initialize a list to store knowledge triples\n",
    "knowledge_triples = []\n",
    "\n",
    "# Iterate over the edges to extract triples\n",
    "for source, target, data in graph.edges(data=True):\n",
    "    # Check if the 'title' key exists in the edge data\n",
    "    if 'title' in data:\n",
    "        relationship = data['title']\n",
    "        triple = KnowledgeTriple(source, relationship, target)\n",
    "        knowledge_triples.append(triple)\n",
    "\n",
    "# Display the extracted knowledge triples\n",
    "print(f\"Extracted {len(knowledge_triples)} knowledge triples:\")\n",
    "print(f\"example knowledge triple: {knowledge_triples[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
    "# langchain_graphgraph = NetworkxEntityGraph()\n",
    "\n",
    "# #make langchain KnowledgeTriple from extracted triples\n",
    "# for triple in knowledge_triples:\n",
    "#     langchain_graphgraph.add_triple(triple)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphRAG using langchain package\n",
    "_Note: only works with langchain generated graph_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_graph = langchain_nxe_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Successfully initialized NetworkxEntityGraph.\")\n",
    "# #get number of nodes and edges\n",
    "# print(f\"Number of nodes: {langchain_graph.get_number_of_nodes()}\")\n",
    "\n",
    "# #get test information about an entity\n",
    "# #get an entity from the graph\n",
    "# list_triples = langchain_graph.get_triples()\n",
    "# print(f\"example triple: {list_triples[-1:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template_mod = \"\"\"\n",
    "# Student Profile:\n",
    "# - Name: {student_name}\n",
    "# - Learning Objectives: {learning_objectives}\n",
    "# - Profile Details: {student_profile}\n",
    "\n",
    "# Tasks:\n",
    "# 1. Based on the provided profile and learning objectives, determine the optimal learning path(s) (including order) to achieve all objectives.\n",
    "# 2. Identify and list specific content aligns with and supports the optimal path(s). Use additional context if provided.\n",
    "# 3. Suggest alternative or backup content that can replace the primary content identified in case of availability issues or better alignment with the student's preferences.\n",
    "# 4. Respond to the request with text output.\n",
    "\n",
    "# Request: \n",
    "# {learning_request}\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import GraphQAChain\n",
    "# #create graphQAchain\n",
    "# chain = GraphQAChain.from_llm(\n",
    "#     llm=llm, \n",
    "#     graph=langchain_graph, \n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# #test\n",
    "# formatted_prompt = prompt_template_mod.format(\n",
    "#         student_name=student_1[\"name\"],\n",
    "#         learning_objectives=student_1[\"learning_objectives\"],\n",
    "#         student_profile=student_1[\"profile\"],\n",
    "#         learning_request=\"Generate an individualized learning plan tailored to the student's needs.\",\n",
    "#     )\n",
    "\n",
    "\n",
    "# chain.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import GraphQAChain\n",
    "\n",
    "# with open(output_path, \"a\") as file:\n",
    "#     file.write(\"\\n-------------Generated with Langchain GraphRAG (mod template 1)-------------\\n\")\n",
    "\n",
    "# with open(context_review_path, \"a\") as file:\n",
    "#     file.write(\"\\n-------------Generated with Langchain GraphRAG (mod template 1)-------------\\n\")\n",
    "\n",
    "# for student in students:\n",
    "    \n",
    "#     #create graphQAchain\n",
    "#     chain = GraphQAChain.from_llm(\n",
    "#         llm=llm, \n",
    "#         graph=langchain_graph, \n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     formatted_prompt = prompt_template_mod.format(\n",
    "#             student_name=student[\"name\"],\n",
    "#             learning_objectives=student[\"learning_objectives\"],\n",
    "#             student_profile=student[\"profile\"],\n",
    "#             learning_request=\"Generate an individualized learning plan tailored to the student's needs.\",\n",
    "#         )\n",
    "\n",
    "#     chain.invoke(formatted_prompt)\n",
    "\n",
    "#     with open(output_path, \"a\") as file:\n",
    "#         file.write(\"\\n------ \"+student[\"test_case\"]+\"\\n\"+ response['answer'] + \"\\n\\n\")\n",
    "\n",
    "#     with open(context_review_path, \"a\") as file:\n",
    "#         file.write(\"\\n------ \"+student[\"test_case\"]+\"\\n\"+ response['context'] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG  custom with community summaries #TODO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from https://github.com/stephenc222/example-graphrag/\n",
    "# # 5. Graph Communities → Community Summaries\n",
    "# def detect_communities(graph):\n",
    "#     communities = []\n",
    "#     index = 0\n",
    "#     for component in nx.connected_components(graph):\n",
    "#         print(\n",
    "#             f\"Component index {index} of {len(list(nx.connected_components(graph)))}:\")\n",
    "#         subgraph = graph.subgraph(component)\n",
    "#         if len(subgraph.nodes) > 1:  # Leiden algorithm requires at least 2 nodes\n",
    "#             try:\n",
    "#                 sub_communities = algorithms.leiden(subgraph)\n",
    "#                 for community in sub_communities.communities:\n",
    "#                     communities.append(list(community))\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing community {index}: {e}\")\n",
    "#         else:\n",
    "#             communities.append(list(subgraph.nodes))\n",
    "#         index += 1\n",
    "#     print(\"Communities from detect_communities:\", communities)\n",
    "#     return communities\n",
    "\n",
    "# def summarize_communities(communities, graph):\n",
    "#     community_summaries = []\n",
    "#     for index, community in enumerate(communities):\n",
    "#         print(f\"Summarize Community index {index} of {len(communities)}:\")\n",
    "#         subgraph = graph.subgraph(community)\n",
    "#         nodes = list(subgraph.nodes)\n",
    "#         edges = list(subgraph.edges(data=True))\n",
    "#         description = \"Entities: \" + \", \".join(nodes) + \"\\nRelationships: \"\n",
    "#         relationships = []\n",
    "#         for edge in edges:\n",
    "#             relationships.append(\n",
    "#                 f\"{edge[0]} -> {edge[2]['label']} -> {edge[1]}\")\n",
    "#         description += \", \".join(relationships)\n",
    "\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"Summarize the following community of entities and relationships.\"},\n",
    "#                 {\"role\": \"user\", \"content\": description}\n",
    "#             ]\n",
    "#         )\n",
    "#         summary = response.choices[0].message.content.strip()\n",
    "#         community_summaries.append(summary)\n",
    "#     return community_summaries\n",
    "\n",
    "\n",
    "# # 6. Community Summaries → Community Answers → Global Answer\n",
    "# def generate_answers_from_communities(community_summaries, query):\n",
    "#     intermediate_answers = []\n",
    "#     for index, summary in enumerate(community_summaries):\n",
    "#         print(f\"Summary index {index} of {len(community_summaries)}:\")\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=\"gpt-4o\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"Answer the following query based on the provided summary.\"},\n",
    "#                 {\"role\": \"user\", \"content\": f\"Query: {query} Summary: {summary}\"}\n",
    "#             ]\n",
    "#         )\n",
    "#         print(\"Intermediate answer:\", response.choices[0].message.content)\n",
    "#         intermediate_answers.append(\n",
    "#             response.choices[0].message.content)\n",
    "\n",
    "#     final_response = client.chat.completions.create(\n",
    "#         model=\"gpt-4o\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\",\n",
    "#                 \"content\": \"Combine these answers into a final, concise response.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Intermediate answers: {intermediate_answers}\"}\n",
    "#         ]\n",
    "#     )\n",
    "#     final_answer = final_response.choices[0].message.content\n",
    "#     return final_answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
