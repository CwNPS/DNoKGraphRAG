{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph extraction from documents\n",
    "Based on https://github.com/lamm-mit/GraphReasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a knowledge graph using functions in the GraphReasoning_Mod module which is adpated from https://github.com/lamm-mit/GraphReasoning to work with huggingface endpoints and for use with huggingface documents. \n",
    "- documents are scrapped using data.ipynb from pdfs, blogs, and youtube videos transcripts. \n",
    "- LLM model Mistral-Nemo-Instruct-2407\n",
    "- Embedding model: dunzhang/stella_en_1.5B_v5 (chosen based on size and position on leaderboard DEC 2024)\n",
    "- KG created without refinement loops in when identifying nodes. \n",
    "- KG uses simplify graph and additonal tools in GraphReasoning_Mod\n",
    "\n",
    "- KG has metadata along edges and uses an updated generation prompt.\n",
    "- KG does not keep source documents as nodes (could add later by referencing the pre-simplify.csv) \n",
    "\n",
    "_Note:Some file paths may have to be updated due to restructing of the repo_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "root_path = \"c:\\\\Users\\\\jonathan.kasprisin\\\\gitlab\\\\DNoK_GraphRAG\"\n",
    "os.chdir(root_path)\n",
    "sys.path.append(root_path)\n",
    "\n",
    "from GraphReasoning_Mod.graph_tools import *\n",
    "from GraphReasoning_Mod.utils import *\n",
    "from GraphReasoning_Mod.graph_generation import *\n",
    "from GraphReasoning_Mod.graph_analysis import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "#Initialize the model endpoint\n",
    "HOST_URL_INF = \":8080\" #Mistral-NeMo-Instruct-2407\n",
    "MAX_NEW_TOKENS = 1012\n",
    "\n",
    "TEMPERATURE = 0.2\n",
    "TIMEOUT = 120\n",
    "TOP_P = .9\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=HOST_URL_INF,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,\n",
    "    temperature = TEMPERATURE,\n",
    "    timeout=TIMEOUT,\n",
    "    top_p=TOP_P\n",
    ")\n",
    "#print(llm.invoke(\"What is HuggingFace?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"dunzhang/stella_en_1.5B_v5\" #\"BAAI/bge-small-en-v1.5\" #dunzhang/stella_en_1.5B_v5\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embd = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# load pickled documents\n",
    "pickle_file_path = './data/storage/full_all_documents.pkl'\n",
    "if os.path.exists(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        all_pdf_docs, all_yt_docs, all_blog_docs = pickle.load(f)\n",
    "else:\n",
    "    print(\"Pickle file not found.\")\n",
    "\n",
    "#check if the documents are loaded\n",
    "print(\"Number of PDF documents:\", len(all_pdf_docs))\n",
    "print(\"Number of YouTube documents:\", len(all_yt_docs))\n",
    "print(\"Number of blog documents:\", len(all_blog_docs))\n",
    "\n",
    "\n",
    "#standardize the metadata\n",
    "all_pdf_docs, all_yt_docs, all_blog_docs = standardize_document_metadata(all_pdf_docs, all_yt_docs, all_blog_docs)\n",
    "\n",
    "# Combine all documents into a single list\n",
    "all_docs = all_pdf_docs+  all_yt_docs+  all_blog_docs\n",
    "\n",
    "print(f\"Total number of documents: {len(all_docs)}\")\n",
    "\n",
    "#print dictionary keys from metadata\n",
    "print(\"Metadata keys for PDF documents:\", all_pdf_docs[0].metadata.keys())\n",
    "print(\"Metadata keys for yt documents:\", all_yt_docs[0].metadata.keys())\n",
    "print(\"Metadata keys for blog documents:\", all_blog_docs[0].metadata.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create create and save networkx graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize variables\n",
    "# G_existing = None\n",
    "# existing_node_embeddings = None\n",
    "# failed_batches = []\n",
    "# output_directory = 'data_no_refine3'  \n",
    "\n",
    "# with open(f'{output_directory}/embeddings.pkl', 'rb') as f:\n",
    "#     existing_node_embeddings = pickle.load(f)\n",
    "\n",
    "# with open(f'{output_directory}/failed_batches.pkl', 'rb') as f:\n",
    "#     failed_batches = pickle.load(f)\n",
    "\n",
    "# graph_path=f'{output_directory}/final_augmented_graph.graphml'\n",
    "# G_existing = nx.read_graphml(graph_path)\n",
    "    \n",
    "\n",
    "\n",
    "# # Process documents in chunks\n",
    "# chunk_size = 1500\n",
    "# batch_size = 5\n",
    "\n",
    "# # Split all_docs into batches of size batch_size\n",
    "# doc_batches = [all_docs[i:i + batch_size] for i in range(0, len(all_docs), batch_size)]\n",
    "\n",
    "# doc_batches = doc_batches[453:]\n",
    "\n",
    "# for batch_idx, doc_batch in tqdm(enumerate(doc_batches), total=len(doc_batches), desc=\"Processing batches...\"):\n",
    "#     try:\n",
    "#         G_existing, existing_node_embeddings, res = add_new_subgraph_from_docs(\n",
    "#             input_docs=doc_batch,\n",
    "#             llm=llm,\n",
    "#             embd=embd,\n",
    "#             data_dir_output=f\"./{output_directory}/\",\n",
    "#             verbatim=False,\n",
    "#             size_threshold=10,\n",
    "#             chunk_size=chunk_size,\n",
    "#             do_Louvain_on_new_graph=True,\n",
    "#             include_contextual_proximity=False,\n",
    "#             repeat_refine=0,\n",
    "#             similarity_threshold=1.0,\n",
    "#             do_simplify_graph=False,\n",
    "#             return_only_giant_component=False,\n",
    "#             save_common_graph=False,\n",
    "#             G_exisiting=G_existing,\n",
    "#             graph_GraphML_exisiting=None,\n",
    "#             existing_node_embeddings=existing_node_embeddings,\n",
    "#             add_source_nodes=False\n",
    "#         )\n",
    "\n",
    "#         print(f\"Processed batch {batch_idx}, updated graph stats:\", res)\n",
    "#         with open(f'{output_directory}/embeddings.pkl', 'wb') as f:\n",
    "#             pickle.dump(existing_node_embeddings, f)\n",
    "#         with open(f'{output_directory}/failed_batches.pkl', 'wb') as f:\n",
    "#             pickle.dump(failed_batches, f)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         # Log the failed batch index\n",
    "#         failed_batches.append(batch_idx)\n",
    "#         print(f\"Error processing batch {batch_idx} with batch size {batch_size}: {e}\")\n",
    "#         traceback.print_exc()\n",
    "\n",
    "#         with open(f'{output_directory}/error_log.txt', 'a', encoding='utf-8') as f:\n",
    "#             f.write(f\"Batch {batch_idx} failed. Error: {e}\\n\")\n",
    "\n",
    "# # Final graph statistics and saving\n",
    "# print(\"Final graph statistics:\", res if 'res' in locals() else \"No successful batches\")\n",
    "# print(\"Failed batch indices:\", failed_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import tqdm as tqdm\n",
    "\n",
    "def simplify_graph_v2(graph_in, node_embeddings, embd, llm=None, similarity_threshold=0.95, use_llm=False,\n",
    "                   data_dir_output='./', graph_root='simple_graph', verbatim=False, max_tokens=2048, \n",
    "                   temperature=0.3, generate=None):\n",
    "    \"\"\"\n",
    "    Simplifies a graph by merging similar nodes. Modified for large graphs and memory constraints\n",
    "    \"\"\"\n",
    "    if verbatim:\n",
    "            print(\"calculating KNN...\")\n",
    "\n",
    "    graph = graph_in.copy()\n",
    "    \n",
    "    nodes = list(node_embeddings.keys())\n",
    "    embeddings_matrix = np.array([np.array(node_embeddings[node]).flatten() for node in nodes])\n",
    "\n",
    "    assert len(nodes) == embeddings_matrix.shape[0], \"simplify_graph: Number of nodes and embeddings do not match\"\n",
    "    \n",
    "    #check if any node embeddings are None\n",
    "    none_in_embeddings = False\n",
    "    if None in embeddings_matrix:\n",
    "        none_in_embeddings = True\n",
    "\n",
    "        \n",
    "    assert not none_in_embeddings, \"simplify_graph: None in embeddings\"\n",
    "\n",
    "     # Using NearestNeighbors to reduce memory usage\n",
    "    nn = NearestNeighbors(metric='cosine', n_neighbors=30, n_jobs=-1)  # n_neighbors > 1 to compare\n",
    "    nn.fit(embeddings_matrix)\n",
    "    distances, indices = nn.kneighbors(embeddings_matrix)\n",
    "\n",
    "\n",
    "    node_mapping = {}\n",
    "    nodes_to_recalculate = set()\n",
    "    merged_nodes = set()  # Keep track of nodes that have been merged\n",
    "\n",
    "    if verbatim:\n",
    "        print(\"Start merge\")\n",
    "\n",
    "    for i, neighbors in tqdm.tqdm(enumerate(indices), total=len(nodes), desc=\"Merging nodes...\"):\n",
    "        node_i = nodes[i]\n",
    "        for j, dist in zip(neighbors[1:], distances[i][1:]):  # Skip self (index 0 is itself)\n",
    "            if dist <= (1 - similarity_threshold):\n",
    "                node_j = nodes[j]\n",
    "                if node_i != node_j and node_j not in merged_nodes and node_i not in merged_nodes:\n",
    "                    try:\n",
    "                        if graph.degree(node_i) >= graph.degree(node_j):\n",
    "                            node_to_keep, node_to_merge = node_i, node_j\n",
    "                        else:\n",
    "                            node_to_keep, node_to_merge = node_j, node_i\n",
    "\n",
    "                        # if verbatim:\n",
    "                        #     print(f\"Merging: {node_to_merge} --> {node_to_keep}\")\n",
    "\n",
    "                        node_mapping[node_to_merge] = node_to_keep\n",
    "                        nodes_to_recalculate.add(node_to_keep)\n",
    "                        merged_nodes.add(node_to_merge)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error merging nodes {node_i} and {node_j}: {e}\")\n",
    "    if verbatim:\n",
    "        print (\"Now relabel. \")\n",
    "    # Create the simplified graph by relabeling nodes. removes adds edges from to_merge to to_keep and removes to_merge.\n",
    "    new_graph = nx.relabel_nodes(graph, node_mapping, copy=True)\n",
    "    if verbatim:\n",
    "        print (\"New graph generated, nodes relabled. \")\n",
    "    # Recalculate embeddings for nodes that have been merged or renamed.\n",
    "    recalculated_embeddings = regenerate_node_embeddings(new_graph, nodes_to_recalculate, embd, verbatim=verbatim)\n",
    "    if verbatim:\n",
    "        print (\"Relcaulated embeddings... \")\n",
    "    # Update the embeddings dictionary with the recalculated embeddings.\n",
    "    updated_embeddings = {**node_embeddings, **recalculated_embeddings}\n",
    "\n",
    "    # Remove embeddings for nodes that no longer exist in the graph.\n",
    "    for node in merged_nodes:\n",
    "        updated_embeddings.pop(node, None)\n",
    "    if verbatim:\n",
    "        print (\"Now save graph... \")\n",
    "\n",
    "    # Save the simplified graph to a file.\n",
    "    graph_path = f'{data_dir_output}/{graph_root}_graphML_simplified.graphml'\n",
    "    nx.write_graphml(new_graph, graph_path)\n",
    "\n",
    "    if verbatim:\n",
    "        print(f\"Graph simplified and saved to {graph_path}\")\n",
    "\n",
    "    return new_graph, updated_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len nodes: 77039 \n",
      "len embeddings: 77039\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "output_directory = 'data_no_refine3'  \n",
    "\n",
    "with open(f'{output_directory}/embeddings.pkl', 'rb') as f:\n",
    "    existing_node_embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "graph_path=f'{output_directory}/final_augmented_graph.graphml'\n",
    "G_existing = nx.read_graphml(graph_path)\n",
    "\n",
    "nodes = list(existing_node_embeddings.keys())\n",
    "embeddings_matrix = np.array([np.array(existing_node_embeddings[node]).flatten() for node in nodes])\n",
    "\n",
    "print(f\"Len nodes: {len(nodes)} \\nlen embeddings: {len(embeddings_matrix)}\") \n",
    "\n",
    "#check if any node embeddings are None\n",
    "if None in embeddings_matrix:\n",
    "    print( \"simplify_graph: None in embeddings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating KNN...\n",
      "Start merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging nodes...: 100%|██████████| 77039/77039 [00:00<00:00, 82362.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now relabel. \n",
      "New graph generated, nodes relabled. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recalculating embeddings: 100%|██████████| 14974/14974 [1:41:08<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relcaulated embeddings... \n",
      "Now save graph... \n",
      "Graph simplified and saved to data_no_refine3/0.95threshold_graphML_simplified.graphml\n",
      "simple graph statistics: ({'Number of Nodes': 37709, 'Number of Edges': 58504, 'Average Degree': 3.102919727386035, 'Density': 8.228810139455911e-05, 'Connected Components': 3171, 'Number of Communities': 3228}, False)\n"
     ]
    }
   ],
   "source": [
    "import tqdm as tqdm\n",
    "try:\n",
    "\n",
    "    #simplify to .95 threshold\n",
    "    simplified_graph, simplified_embeddings = simplify_graph_v2(G_existing, existing_node_embeddings, embd, similarity_threshold=0.95, graph_root='0.95threshold', data_dir_output=output_directory, verbatim=True)\n",
    "    \n",
    "    with open(f'{output_directory}/0.95threshold_embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(simplified_embeddings, f)\n",
    "\n",
    "    res= graph_statistics_and_plots_for_large_graphs(\n",
    "            simplified_graph, data_dir=output_directory,\n",
    "            include_centrality=False, make_graph_plot=False,\n",
    "            root='simple_graph')\n",
    "\n",
    "    print(\"simple graph statistics:\", res ) \n",
    "except Exception as e:\n",
    "    print(f\"Error simplifying graph: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple graph statistics: ({'Number of Nodes': 18933, 'Number of Edges': 33153, 'Average Degree': 3.5021391221676437, 'Density': 0.00018498516385842193, 'Connected Components': 510, 'Number of Communities': 639}, False)\n"
     ]
    }
   ],
   "source": [
    "output_directory = \"./data/generated_graphs/GR_no_refine3/\"\n",
    "with open(f'{output_directory}/0.95threshold_embeddings.pkl', 'rb') as f:\n",
    "    simplified_embeddings = pickle.load(f)\n",
    "\n",
    "simplified_graph = nx.read_graphml(f'{output_directory}/0.95threshold_graphML_simplified.graphml')\n",
    "\n",
    "#simplify to .85 threshold\n",
    "simplified_graph, simplified_embeddings = simplify_graph(simplified_graph, simplified_embeddings, embd, similarity_threshold=0.85, graph_root='0.85threshold', data_dir_output=output_directory)\n",
    "\n",
    "print(f\"-->number of embeddings: {len(simplified_embeddings)}\")\n",
    "simplified_embeddings = update_node_embeddings(simplified_graph, simplified_embeddings, embd, remove_embeddings_for_nodes_no_longer_in_graph=True)\n",
    "\n",
    "print(f\"-->number of embeddings after removed nodes: {len(simplified_embeddings)}\")\n",
    "with open(f'{output_directory}/0.85threshold_embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(simplified_embeddings, f)\n",
    "\n",
    "res= graph_statistics_and_plots_for_large_graphs(\n",
    "            simplified_graph, data_dir=output_directory,\n",
    "            include_centrality=False, make_graph_plot=False,\n",
    "            root='simple_graph')\n",
    "\n",
    "print(\"simple graph statistics:\", res ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple graph statistics: ({'Number of Nodes': 13596, 'Number of Edges': 24305, 'Average Degree': 3.5753162694910268, 'Density': 0.00026298758878198064, 'Connected Components': 199, 'Number of Communities': 280}, False)\n"
     ]
    }
   ],
   "source": [
    "#simplify to .75 threshold\n",
    "simplified_graph, simplified_embeddings = simplify_graph(simplified_graph, simplified_embeddings, embd, similarity_threshold=0.75, graph_root='0.75threshold', data_dir_output=output_directory)\n",
    "\n",
    "print(f\"-->number of embeddings: {len(simplified_embeddings)}\")\n",
    "simplified_embeddings = update_node_embeddings(simplified_graph, simplified_embeddings, embd, remove_embeddings_for_nodes_no_longer_in_graph=True)\n",
    "\n",
    "print(f\"-->number of embeddings after removed nodes: {len(simplified_embeddings)}\")\n",
    "\n",
    "with open(f'{output_directory}/0.75threshold_embeddings.pkl', 'wb') as f:\n",
    "            pickle.dump(simplified_embeddings, f)\n",
    "\n",
    "res= graph_statistics_and_plots_for_large_graphs(\n",
    "            simplified_graph, data_dir=output_directory,\n",
    "            include_centrality=False, make_graph_plot=False,\n",
    "            root='simple_graph')\n",
    "\n",
    "print(\"simple graph statistics:\", res ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_ilp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
