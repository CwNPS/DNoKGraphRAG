{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Generation using Langchain libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup llm and graphdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "# config for endpoints\n",
    "#config\n",
    "HOST_URL_INF = \":8080\"\n",
    "MAX_NEW_TOKENS = 2024\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=HOST_URL_INF,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    do_sample=False,\n",
    ")\n",
    "#print(llm.invoke(\"What is HuggingFace?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDF documents: 2048\n",
      "Number of YouTube documents: 442\n",
      "Number of blog documents: 11\n",
      "Total number of documents: 2501\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# load pickled documents\n",
    "pickle_file_path = '../../data/storage/full_all_documents.pkl'\n",
    "if os.path.exists(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        all_pdf_docs, all_yt_docs, all_blog_docs = pickle.load(f)\n",
    "else:\n",
    "    print(\"Pickle file not found.\")\n",
    "\n",
    "#check if the documents are loaded\n",
    "print(\"Number of PDF documents:\", len(all_pdf_docs))\n",
    "print(\"Number of YouTube documents:\", len(all_yt_docs))\n",
    "print(\"Number of blog documents:\", len(all_blog_docs))\n",
    "\n",
    "all_docs = all_pdf_docs + all_yt_docs + all_blog_docs\n",
    "print(\"Total number of documents:\", len(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(all_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph from documents each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting documents to graph documents: 100%|██████████| 2501/2501 [10:02:49<00:00, 14.46s/doc]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2479 documents to graph documents.\n",
      "Nodes:[]\n",
      "Relationships:[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "dir= \"../../data/langchain_KG/\"\n",
    "error_log_path = dir+\"generation_error_log.txt\"\n",
    "\n",
    "#create directory if it does not exist\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "#Initalize error log file\n",
    "with open(error_log_path, \"w\") as f:\n",
    "    f.write(\"Error log for langchain graph document generation\\n\")\n",
    "\n",
    "# Convert documents to graph documents with a progress bar\n",
    "graph_documents = []\n",
    "for doc in tqdm(all_docs, desc=\"Converting documents to graph documents\", unit=\"doc\"):\n",
    "    try:\n",
    "        graph_doc = llm_transformer.convert_to_graph_documents([doc])\n",
    "        graph_documents.extend(graph_doc)\n",
    "    except Exception as e:\n",
    "        with open(error_log_path, \"a\") as f:\n",
    "            f.write(f\" 'doc.metadata': '{doc.metadata}', 'error': {e}\\n\")\n",
    "        \n",
    "\n",
    "print(f\"Converted {len(graph_documents)} documents to graph documents.\")\n",
    "\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle graph documents\n",
    "pickle_file_path = dir + 'full_graph_documents.pkl'\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(graph_documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes=[] relationships=[] source=Document(metadata={'source': 'C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'page': 1}, page_content='Linear Algebra and Its Applications\\nFourth Edition\\nGilbert Strang\\ny\\nx y z \\x1e \\x0c \\nz\\nAx b\\x1e\\nb\\n0\\nAy b\\x1e\\n0Az \\x1e\\n0')\n",
      "{'source': 'C:\\\\Users\\\\jonathan.kasprisin\\\\github\\\\Learning\\\\KG_ilp\\\\data\\\\pdfs\\\\Gilbert_Strang_Linear_Algebra_and_Its_Applicatio_230928_225121.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "print(graph_documents[1])\n",
    "print(graph_documents[1].source.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph with networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store to graph\n",
    "# Filter out nodes with empty labels\n",
    "filtered_graph_documents = []\n",
    "for doc in graph_documents:\n",
    "    valid_nodes = [node for node in doc.nodes if node.type]\n",
    "    if valid_nodes:\n",
    "        doc.nodes = valid_nodes\n",
    "        filtered_graph_documents.append(doc)\n",
    "    else:\n",
    "        with open(dir+\"dropped_docs.txt\", \"a\") as f:\n",
    "            f.write(f\" 'Dropped doc.metadata': '{doc.source.metadata}'\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 983 documents to graph documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filtered {len(filtered_graph_documents)} documents to graph documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making nx graph from 983 graph documents\n",
      "nx graph built with 10177 nodes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def make_nxe_graph(graph_documents) -> nx.Graph:\n",
    "    print(f\"Making nx graph from {len(graph_documents)} graph documents\")\n",
    "    graph_nxe = NetworkxEntityGraph()\n",
    "    for doc in graph_documents:\n",
    "        try:\n",
    "            for node in doc.nodes:\n",
    "                graph_nxe.add_node(node.id)\n",
    "            for edge in doc.relationships:\n",
    "                graph_nxe._graph.add_edge(edge.source.id, edge.target.id, relation=edge.type)\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document to nx graph: {doc.source.metadata}, {e}\")\n",
    "    print(f\"nx graph built with {graph_nxe.get_number_of_nodes()} nodes.\") \n",
    "    return graph_nxe\n",
    "\n",
    "\n",
    "graph_nxe = make_nxe_graph(filtered_graph_documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example triple: ('Positive Definite Matrices', 'Minima, Maxima, and Saddle Points', 'INCLUDES')\n",
      "Example node: Positive Definite Matrices\n",
      "Example node knowledge: ['Positive Definite Matrices INCLUDES Minima, Maxima, and Saddle Points', 'Positive Definite Matrices INCLUDES Tests for Positive Definiteness', 'Positive Definite Matrices INCLUDES Singular Value Decomposition', 'Singular Value Decomposition DESCRIBED_BY The SVD is closely associated with the eigenvalue-eigenvector factorizationQΛQT of a positive deﬁnite matrix', 'Singular Value Decomposition IS_EXPLAINED_IN Proof of the Singular Value Decomposition', 'Positive Definite Matrices INCLUDES Minimum Principles', 'Positive Definite Matrices INCLUDES The Finite Element Method']\n"
     ]
    }
   ],
   "source": [
    "#see example node and relationships\n",
    "node = 1\n",
    "\n",
    "# Try to get node data (adjust method name as needed)\n",
    "try:\n",
    "    triples = graph_nxe.get_triples()\n",
    "    print(f\"Example triple: {triples[0]}\")\n",
    "    entity= triples[0][0]\n",
    "    print(f\"Example node: {entity}\")\n",
    "    knowledge = graph_nxe.get_entity_knowledge(entity, 3)\n",
    "    print(f\"Example node knowledge: {knowledge}\")\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Unable to access node data. Check the class documentation for the correct method.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save graph\n",
    "graph_file_path = dir+'langchain_full_kg.graphml'\n",
    "nx.write_graphml(graph_nxe._graph, graph_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
